+++
title  = "Fast reciprocal square root... in 1997?!"
date = "2021-05-06"
tags = ["i76"]
draft = true
+++

This article is part of my series on reverse engineering [Interstate '76](https://en.wikipedia.org/wiki/Interstate_%2776), with my
current goal being to add a [Vulkan](https://en.wikipedia.org/wiki/Vulkan_(API)) renderer to the game.  I'm basing this work
directly on [UCyborg's patches](https://community.pcgamingwiki.com/files/file/1349-interstate-76-nitro-pack-aio-patch/) which include
many much-needed fixups to the game, including [my own patches]({{< relref "projects/interstate76anet.md" >}}) for the netcode.

# Introduction

Everyone is familiar with the famous [fast reciprocal square root function in the Quake
3 source code](https://en.wikipedia.org/wiki/Fast_inverse_square_root).  And, as noted
on Wikipedia, solutions have existed for computing the fast reciprocal square root
for many years before that, with perhaps the earliest implementation in 1986.

While I was working on reverse engineering Interstate '76, I discovered that Activision
had their own implementation in use from at least 1997, and that it shares some similarities
with the Quake 3 approach.  Interstate '76 is based off the Mechwarrior 2 engine --
If I had a copy of Mechwarrior 2 (DOS version) on hand, I could confirm
if this was present there as well, which would date this technique even
earlier!  In this post, I'll explain how it works.

# The TL;DR

The approach used in the Mechwarrior 2 engine computes an approximation of the reciprocal
square root using the x87 instruction set at `float64` (or `double`) precision.

It is almost exactly the same as the Quake 3 approach except that the initial guess is computed differently.
It still uses Newton-Raphson with a few manual adjustments. The function seems to be
accurate within 4 significant digits in the worst case from some brief testing I've done.

The initial guess is computed a Lookup Table ([LUT](https://en.wikipedia.org/wiki/Lookup_table)) to
obtain mantissa bits.  It is best illustrated with the following C++20 code:

{{< highlight cpp >}}
uint8_t LUT[256];

void generateLUT()
{
    for (uint32_t i = 0; i < 256; ++i)
    {
        uint64_t const float64bits = (static_cast<uint64_t>(i) | UINT64_C(0x1ff00)) << 0x2d;
        double const d = std::bit_cast<double>(float64bits);
        double const rsqrt = 1.0 / std::sqrt(d);
        uint64_t const u64rsqrt = std::bit_cast<uint64_t>(rsqrt);
        uint32_t const high32bits = static_cast<uint32_t>(u64rsqrt >> 0x20);

        //"round up" operation: add 010000000000b (1 << 10): lower bits of mantissa are thrown out.
        //this has the effect of correctly rounding the mantissa within 8 bits
        //using the "round up" rule at the 10-bit mark.
        uint32_t const high32bits_rounded_up = high32bits + 0x400;
        uint8_t const mantissa_high8bits_only = (high32bits_rounded_up >> 0xc) & UINT32_C(0xFF);

        //store the 8 bits of mantissa remaining
        LUT[i] = mantissa_high8bits_only;
    }
    LUT[0x80] = 0xFF;
}
{{< /highlight >}}

The exponent of the guess is determined using a formula that operates directly on the exponent field of the input itself.
The LUT has 256 entries, each corresponding a `float64` in the range $[0.5,2)$:
  * $[0.5,1)$ iterating at a granularity of $0.00390625$ (the first 128 entries)
  * $[1,2)$ iterating at a granularity of $0.0078125$ (the second half of the LUT)
Each entry contains the 8 most significant mantissa bits of the reciprocal square root for the `float64` corresponding to that entry.
It is manually adjusted to create more desirable characteristics when approximating the reciprocal square root of a `float64` in the range of
$[1,2)$.

Note that the LUT wasn't precomputed and stored as a static array in the binary -- it was generated in the initialization
sequence of the engine.  To be honest, 
I'm kind of glad -- it would have made this analysis a little harder to do!
The code above could easily be made `constexpr` with a few tweaks.

The actual fast reciprocal square root function computes the result as follows:

1. Compute an initial guess for the result:
    * the exponent bits for the initial guess is determined through a formula that involves only addition and division by 2 (bit shift)
    * the mantissa bits for the initial guess is determined using the lookup table
    * `or` these values together to form a valid `float64` -- this is the guess
2. Perform one iteration of Newton-Raphson on the initial guess
3. Multiply the result by the constant $1.00001$

Below is my own revese engineered C++20 version of this function:

{{< highlight cpp >}}
double i76_rsqrt(double const scalar)
{
    uint64_t const scalar_bits = std::bit_cast<uint64_t>(scalar);

    uint8_t const index = (scalar_bits >> 0x2d) & 0xff;

    //LUT[index] contains the 8 most significant bits of the mantissa, rounded up.
    //Treat all lower 44 bits as zeroed out
    uint64_t const mantissa_bits = static_cast<uint64_t>(LUT[index]) << 0x2c;
   
    //Exponent bits are calculated based on the formula in the article
    uint64_t const exponent_bits = ((0xbfcU - (scalar_bits >> 0x34)) >> 1) << 0x34;
    
    //exponent_bits have form 0xYYY00000 00000000
    //mantissa_bits have form 0x000ZZ000 00000000
    //so combined, we have    0xYYYZZ000 00000000 -- a complete float64 for the guess
    uint64_t const combined_bits = exponent_bits | mantissa_bits;

    auto const initial_guess = std::bit_cast<double>(combined_bits);

    auto const half_initial_guess = initial_guess * 0.5;
    auto const initial_guess_squared = initial_guess * initial_guess;
    
    //One iteration of Newton-Raphson
    auto const newton_raphson = (3.0 - (scalar * initial_guess_squared)) * half_initial_guess;

    //post-hoc fixup
    auto const fixup = newton_raphson * 1.00001;

    return fixup; 
}
{{< /highlight >}}

When compiled for x86 using only x87 instructions  on MSVC 2019, it produces nearly identical assembly to the original.
The generated code for these functions is very similar to what the original game offered when built using fast math optimizations.

## Patched version of game with screenshots

TODO

So, I went ahead and replaced this with a much more accurate `rsqrt` in I'76.  Here are the results:
...

TODO PICTURES OF BEFORE AND AFTER

The patched code:
...

# The Long Version

#### Foreword

You know, back when I first started delving into reverse engineering I'76 ten years ago,
tools like [Ghidra](https://ghidra-sre.org/) weren't available.  The state of the art
was probably [IDA Pro](https://www.hex-rays.com/ida-pro/), a tool suite that while reasonably 
priced for what it did, was way out of the price range for me, a third year undergraduate
student.  Sure, pirating it was an option, but if you wanted to do things the honest way *and* on a budget, it was
hard to do better than [Ollydbg](http://www.ollydbg.de/version2.html), a free debugger
and disassembler for 32-bit x86.  Now, ten years later, the landscape looks radically
different.  There are so many fantastic tools available for doing this kind of thing,
and it has never been easier to mod your favourite games as a result.

## Introduction

I first discovered the code when identifying functions in the startup sequence of `nitro.exe` using Ghidra. Some fairly
normal initialization code, then I came across this function that populates a LUT using, seemingly, one byte from the result of a `1.0/sqrt(x)` on a `float64`.
Below is the decompiled version of the function as provided by Ghidra:

{{< highlight cpp >}}
void generate_weird_lut(void)
{
  int uVar1;
  double local_10;
  double var1;
  
  uVar1 = 0;
  do {
    var1._4_4_ = (int)((ulonglong)
      (1.0 / SQRT((double)((ulonglong)((uVar1 | 0x1ff00U) << 0xd) << 0x20))) >> 0x20);
    global_lut[uVar1] = (byte)(var1._4_4_ + 0x400 >> 0xc);
    uVar1 = uVar1 + 1;
  } while (uVar1 < 0x100);
  global_lut[128] = 0xff;
  return;
}
{{< /highlight >}}

Ghidra got it *mostly* right, except the cast to `double`, which is closer to a bitwise cast than a `static_cast`:

{{< highlight nasm >}}
0049AF60 | 83EC 10                  | sub esp,10                              
0049AF63 | 33C0                     | xor eax,eax                             
0049AF65 | 894424 00                | mov dword ptr ss:[esp],eax              
0049AF69 | 8BC8                     | mov ecx,eax                             
0049AF6B | 81C9 00FF0100            | or ecx,1FF00                            
0049AF71 | C1E1 0D                  | shl ecx,D                               
0049AF74 | 894C24 04                | mov dword ptr ss:[esp+4],ecx            
0049AF78 | DD4424 00                | fld st(0),qword ptr ss:[esp]            
0049AF7C | D9FA                     | fsqrt                                   
0049AF7E | DC3D A0504C00            | fdivr st(0),qword ptr ds:[4C50A0]       
0049AF84 | DD5C24 08                | fstp qword ptr ss:[esp+8],st(0)         
0049AF88 | 8B5424 0C                | mov edx,dword ptr ss:[esp+C]            
0049AF8C | 81C2 00040000            | add edx,400                             
0049AF92 | C1FA 0C                  | sar edx,C                               
0049AF95 | 8890 80B15F00            | mov byte ptr ds:[eax+5FB180],dl         
0049AF9B | 40                       | inc eax                                 
0049AF9C | 3D 00010000              | cmp eax,100                             
0049AFA1 | 7C C6                    | jl nitro.49AF69                         
0049AFA3 | C605 00B25F00 FF         | mov byte ptr ds:[5FB200],FF             
0049AFAA | 83C4 10                  | add esp,10                              
0049AFAD | C3                       | ret                                     
{{< /highlight >}}

Only one other function referenced this lookup table, and the code was almost incomprehensible at first glance:

{{< highlight cpp >}}
float10 weird_function(double param_1)
{
  uint uVar1;
  float10 fVar2;
  
  uVar1 = (0xbfc - (param_1._4_4_ >> 0x14) >> 1) << 0x14 |
          (uint)global_lut[param_1._4_4_ >> 0xd & 0xff] << 0xc;
  fVar2 = (float10)(double)((ulonglong)uVar1 << 0x20);
  return ((float10)double_3.0 - fVar2 * fVar2 * (float10)param_1) *
         (float10)(double)((ulonglong)uVar1 << 0x20) * (float10)double_0.5 *
         (float10)double_approx_1.00001;
}
{{< /highlight >}}

Ghidra decompiled this one really well -- the pseudo-C code above is basically a more
readable version of the assembler listing:

{{< highlight nasm >}}
0049C310 | 8B4C24 08                | mov ecx,dword ptr ss:[esp+8]            
0049C314 | BA FC0B0000              | mov edx,BFC                             
0049C319 | C1E9 14                  | shr ecx,14                              
0049C31C | 8B4424 08                | mov eax,dword ptr ss:[esp+8]            
0049C320 | C1E8 0D                  | shr eax,D                               
0049C323 | 2BD1                     | sub edx,ecx                             
0049C325 | D1EA                     | shr edx,1                               
0049C327 | 25 FF000000              | and eax,FF                              
0049C32C | DD4424 04                | fld st(0),qword ptr ss:[esp+4]          
0049C330 | C1E2 14                  | shl edx,14                              
0049C333 | 8A80 80B15F00            | mov al,byte ptr ds:[eax+5FB180]         
0049C339 | C1E0 0C                  | shl eax,C                               
0049C33C | 33C9                     | xor ecx,ecx                             
0049C33E | 0BD0                     | or edx,eax                              
0049C340 | 894C24 04                | mov dword ptr ss:[esp+4],ecx            
0049C344 | 895424 08                | mov dword ptr ss:[esp+8],edx            
0049C348 | DD4424 04                | fld st(0),qword ptr ss:[esp+4]          
0049C34C | DCC8                     | fmul st(0),st(0)                        
0049C34E | DD4424 04                | fld st(0),qword ptr ss:[esp+4]          
0049C352 | D9C9                     | fxch st(0),st(1)                        
0049C354 | DECA                     | fmulp st(2),st(0)                       
0049C356 | DC0D 68115000            | fmul st(0),qword ptr ds:[501168]        
0049C35C | D9C9                     | fxch st(0),st(1)                        
0049C35E | DC2D 60115000            | fsubr st(0),qword ptr ds:[501160]       
0049C364 | DEC9                     | fmulp st(1),st(0)                       
0049C366 | DC0D 70115000            | fmul st(0),qword ptr ds:[501170]        
0049C36C | C3                       | ret                                     
{{< /highlight >}}

Bit shifts, floats
being treated like integers, and other interesting things abound!  It *could* be what I was suspecting, a fast reciprocal square root, but maybe
it was just a small piece of code used for something else.  Not wanting to jump to conclusions, I decided
to study it a bit more.  A good place to start, I figured, would be to see how this mysterious function was used.  I found
fragments of code like this all over the place:

{{< highlight cpp "hl_lines=6">}}

//...

fVar2 = (float)*param_2 - (float)*param_1;
fVar4 = (float)param_2[1] - ((float)param_1[1] - 2.0);
fVar3 = (float)param_2[2] - (float)param_1[2];
fVar8 = weird_function((double)(fVar3 * fVar3 + fVar4 * fVar4 + fVar2 * fVar2));

//...

{{< /highlight >}}

"*Wow, that sure looks like it's computing the [squared length](https://en.wikipedia.org/wiki/Euclidean_distance#Squared_Euclidean_distance) of a 3D vector*," I thought to myself.
If `weird_function` was being called on the results of squared vector lengths, it's almost certain it's being used in vector normalization.  Indeed,
in the fragment of code above, you can see the vector components being multiplied by the result.

Wanting to play with some new tools, I gave Cutter a try with its emulation facilities to run the machine code directly, but unfortunately
couldn't figure out how to get it to show the `ST(i)` registers during execution.  So I moved on to
*x64dbg*, a modern debugger heavily inspired by Ollydbg.

I fired up x64dbg and set a breakpoint for right after where the function where the LUT gets generated is called, then ran it.
Once execution reached the breakpoint, I immediately replaced the instruction pointer to the start of our `weird_function`:

![](start-of-function.png)

Seeing where the argument was expected, I figured why not try evaluating the value `2`?  So, I passed it in
and ran the function to completion:

![](rsqrt2.png)

Indeed, after running the function, `0.7071...` was sitting in `ST(0)`.  That's `1/sqrt(2)` accurate to 4 significant digits.
"*OK fine,*" I thought, "*let's try out some other values too, just to be sure.*"  

So, I went ahead and tried out `100`, `4`, `64`, `0.5`, and `1.2345`:

Trying with **100**: ![](rsqrt100.png)

With **4**: ![](rsqrt4.png)

With **64**: ![](rsqrt64.png)

With **0.5**: ![](rsqrthalf.png)

Finally, with **1.2345**: ![](rsqrt12345.png)

In each case, the result seemed to be accurate within 4 to 6 significant digits.

"*That's it.  I'm convinced!*" I said out loud, "*...but what the hell is it actually doing?!*"

The next step, I figured, would be to decompile it to C++ to help analyze it a bit easier.
This would be a manual process guided by Ghidra's own decompilation, which did a pretty good job, considering.
I wanted to compare it against a correctly rounded reciprocal square root to sample what the error
was across the board.

First, the C++20 version of the LUT generator:

{{< highlight cpp >}}
uint8_t LUT[256];

void generateLUT()
{
    for (uint32_t i = 0; i < 256; ++i)
    {
        uint64_t const float64bits = (static_cast<uint64_t>(i) | UINT64_C(0x1ff00)) << 0x2d;
        double const d = std::bit_cast<double>(float64bits);
        double const rsqrt = 1.0 / std::sqrt(d);
        uint64_t const u64rsqrt = std::bit_cast<uint64_t>(rsqrt);
        uint32_t const high32bits = static_cast<uint32_t>(u64rsqrt >> 0x20);

        //"round up" operation: add 010000000000b (1 << 10): lower bits of mantissa are thrown out.
        //this has the effect of correctly rounding the mantissa within 8 bits
        //using the "round up" rule at the 10-bit mark.
        uint32_t const high32bits_rounded_up = high32bits + 0x400;
        uint8_t const mantissa_high8bits_only = (high32bits_rounded_up >> 0xc) & UINT32_C(0xFF);

        //store the 8 bits of mantissa remaining
        LUT[i] = mantissa_high8bits_only;
    }
    LUT[0x80] = 0xFF;
}
{{< /highlight >}}

If you compile this for 32-bit with MSVC 2019, x87 only, you will get nearly identical assembly to what is in I'76:

{{< highlight nasm "hl_lines=3 21" >}}
; Original I'76                     |   ;New code, MSVC 2019, x86-Release with /arch:IA32 /fp:fast /Oy /O2 /Ot /Ob2 /Oi
sub esp,10                          |   sub         esp,14h  
                                    |   fld1 
xor eax,eax                         |   xor         ecx,ecx  
mov dword ptr ss:[esp],eax          |   mov         eax,ecx  
mov ecx,eax                         |   mov         dword ptr [esp],0  
or ecx,1FF00                        |   shl         eax,0Dh  
shl ecx,D                           |   or          eax,3FE00000h  
mov dword ptr ss:[esp+4],ecx        |   mov         dword ptr [esp+4],eax  
fld st(0),qword ptr ss:[esp]        |   fld         qword ptr [esp]  
fsqrt                               |   fsqrt  
fdivr st(0),qword ptr ds:[4C50A0]   |   fdivr       st,st(1)  
fstp qword ptr ss:[esp+8],st(0)     |   fstp        qword ptr [esp+8]  
mov edx,dword ptr ss:[esp+C]        |   mov         eax,dword ptr [esp+0Ch]  
add edx,400                         |   add         eax,400h  
sar edx,C                           |   shr         eax,0Ch   
mov byte ptr ds:[eax+5FB180],dl     |   mov         byte ptr LUT (0B66678h)[ecx],al 
inc eax                             |   inc         ecx  
cmp eax,100                         |   cmp         ecx,100h  
jl nitro.49AF69                     |   jb          generateLUT+7h (0B61827h)  
                                    |   fstp        st(0)  
mov byte ptr ds:[5FB200],FF         |   mov         byte ptr ds:[0B666F8h],0FFh  
add esp,10                          |   add         esp,14h  
ret                                 |   ret  
{{< /highlight >}}

This is acceptable. The only real difference is the constant `1.0` is obtained via `fld1` rather than a memory reference, and the function cleans it off the x87 stack before returning (this is 
a requirement imposed by the [cdecl](https://en.wikipedia.org/wiki/X86_calling_conventions#cdecl) calling convention). Comparing the LUT generated here with the one visible in x64dbg yielded identical results, so I was confident I got this right.  Now I could actually pick this apart a bit.

You might have noticed that the LUT could have been computed entirely at compile time, or even just stored as
a static array in the executable.  I'm glad that Visual C++ 6.0 wasn't that smart, because it would have made this a fair bit harder to do!

## Some basic analysis of the LUT

The 256-entry LUT looks like this:

<div class="highlight">
<pre class="chroma"><code class="language-nasm" data-lang="nasm">0x00:  6a 68 67 66 64 63 62 60 5f 5e 5c 5b 5a 59 57 56 55 54 53 52 50 4f 4e 4d 4c 4b 4a 49 48 47 46 45
0x20:  44 43 42 41 40 3f 3e 3d 3c 3b 3a 39 38 37 36 35 34 34 33 32 31 30 2f 2f 2e 2d 2c 2b 2a 2a 29 28
0x40:  27 27 26 25 24 24 23 22 21 21 20 1f 1f 1e 1d 1c 1c 1b 1a 1a 19 18 18 17 16 16 15 15 14 13 13 12
0x60:  11 11 10 10 0f 0e 0e 0d 0d 0c 0c 0b 0a 0a 09 09 08 08 07 07 06 05 05 04 04 03 03 02 02 01 01 <span class="kt hl" style="display: inline;">00</span>
0x80:  <span class="kt hl" style="display: inline;">ff</span> fe fc fa f8 f6 f4 f2 f0 ef ed eb e9 e8 e6 e4 e2 e1 df de dc da d9 d7 d6 d4 d3 d1 d0 ce cd cb
0xa0:  ca c8 c7 c5 c4 c3 c1 c0 bf bd bc bb b9 b8 b7 b6 b4 b3 b2 b1 b0 ae ad ac ab aa a8 a7 a6 a5 a4 a3
0xc0:  a2 a1 a0 9f 9e 9c 9b 9a 99 98 97 96 95 94 93 92 91 90 8f 8f 8e 8d 8c 8b 8a 89 88 87 86 85 85 84
0xe0:  83 82 81 80 7f 7f 7e 7d 7c 7b 7a 7a 79 78 77 76 76 75 74 73 73 72 71 70 70 6f 6e 6d 6d 6c 6b 6a</code></pre>
</div>

As you can see, there is a discontinuity between `0x7f` and `0x80`, where it jumps from `00` to `ff`.

I made a spreadsheet to see how this compares to the "precise" result and ended up seeing that
the numerical error actually forms a repeating pattern when viewed on a log scale, which at the time
I thought was interesting.  We'll see why this is the case later. TODO

I tried out the decompiled version of the code by sampling inputs and comparing them with the
correctly rounded "ground truth" result.  The code appears to give about 4 significant digits of precision
in the worst case, sometimes providing 6 or more.  Not bad at all!

But how's this thing actually work?

## IEEE-754 `float64` refresher

Before we go down the numerical analysis path -- a quick refresher on IEEE-754 `float64` (or `double` precision) representations
is due.

A `float64` value is a 64-bit datatype of the following form, where the sign bit is the most significant bit:

{{< figure caption="Image courtesy of Wikipedia" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/IEEE_754_Double_Floating_Point_Format.svg/618px-IEEE_754_Double_Floating_Point_Format.svg.png">}}

Conceptually, you can think of a `float64` like you would a number written in scientific notation,
except in base 2 instead of base 10.
There are certain [special cases of this format](#ieee754-cases) that, for our purposes, we can completely ignore for our analysis.
We assume, because we're dealing with an approximation, that the input is:

* non-`NaN`
* non-`InF` (i.e. finite)
* normal (i.e. not denormal/subnormal)
* positive (i.e., the sign bit is zero)
 
These assumptions simplify the math
and handle the vast majority of use cases that this function would be used for.  Remember:
this thing is supposed to be a fast approximation, not perfect.

There are just two more things to point out before we proceed.  Given a `float64` $v$ conforming to our assumptions:

* The 11-bit exponent for $v$ is stored *biased* by the value $1023$ in the exponent field.  This means to get the true exponent, you need to subtract it by $1023$
* The mantissa for $v$ is actually a 53-bit value.  The lower 52-bits are as they appear in the mantissa field, and the most significant bit is implicitly $1$

For example, the value $-3.625$ is represented as follows.  First, $-3.625 = -2^1(1 + 0.8125)$.  Therefore the unbiased exponent is $1$, and
the biased exponent is $1 + 1023 = 1024$.  The fractional part of the mantissa (lower 52 bits) is 0.8125, as we exclude the implicit leading $1$.
We turn that into a 52-bit integer for the field by multiplying it by $2^{52}$.  Finally, the sign bit is $1$ because the quantity is negative:

~~~
1     10000000000 1101000000000000000000000000000000000000000000000000
^     ^           ^
sign  exponent    mantissa
~~~

In hexadecimal notation, this is written as `0xc00d000000000000`. `float64` is kind of nice because the sign
and exponent bits end on a 4-bit (or *nibble*) boundary, so it's easy to see what the fractional
part of the mantissa is just by looking at the hexadecimal readout.

Working with values this way is a little cumbersome, so let us introduce some notation to make working with `float64`s a little easier.

## Notation

We can represent an IEEE-754 value $v$ that conforms to our assumptions as follows:

{{< katex display >}}
\newcommand{\fexp}{\mathit{exp}}
\newcommand{\ffrac}{\mathit{frac}}
\mathrm{fl}[v] = 2^\fexp(1 + \ffrac)
{{< /katex >}}

where $\mathrm{fl}[v] \in \mathbb{R}$ is the real floating point value, $\fexp \in \mathbb{Z}$ is the unbiased exponent,
and $\ffrac \in \mathbb{R}$ is the fractional part of the mantissa, $0 \leq \ffrac < 1$.

This is a [common notation](http://homepages.math.uic.edu/~hanson/mcs471/FloatingPointRep.html) used in many numerical analysis papers and
will help along our exploration.

Note that this notation places no bounds on precision or range of $fl[v]$.  To actually represent $\mathrm{fl}[v]$ in a `float64`, the following two conditions must hold:

1. $-1023 < \fexp < 1024$
2. $(\exists z \in \mathbb{Z})\ \ffrac = \frac{z}{2^{52}}$

$\ffrac$ can be [rounded](https://en.wikipedia.org/wiki/Floating-point_arithmetic#Rounding_modes) to satisfy condition (2) -- in this case, the `float64` is an approximation
of the real value.

## LUT Generation analysis

First, the loop: the range `[0.5 .. 2)` is iterated 256 (TODO) times using this formula:

~~~
0 1111111111e mmmm mmm 0000 ...
~~~

What this means is half of the iterations are between `[0.5 .. 1)` and half are between `[1 .. 2)`:

* `[0.5 .. 1)` is iterated with an interval of TODO (when `e` is 0)
* `[1 .. 2)` is iterated with an interval of TODO (when `e` is 1)

`1.0/sqrt(x)` is evaluated with each input.  The lower 32 bits of the mantissa are completely thrown away,
The result is something that looks like a `float32`, but with more exponent bits.  This is nicer to work with
because the sign and exponent bits are 12 bits combined, meaning the mantissa is aligned on a nibble (4-bit) boundary.
This makes it very easy to understand when looking at the hexadecimal representation: 

~~~
0x3FE0 0000

  3FE
|    1 2345

sign bit: 0
exponent bits: 0x3FF (1023 biased) (0 unbiased)
mantissa bits: 0x12345
~~~

Each iteration of the loop populates one entry of the LUT.
A `float64` is created by taking this generated bit-pattern and zeroing out the lower 32 bits as follows.
Next, `1.0/sqrt(x)` is evaluated on the FPU to full precision.  The result is obtained as a `float64`
which is then "rounded up" by adding the constant `0x400` to it:

~~~
LUT[i] = to_int32(1.0/sqrt(x)) + 0x400
~~~

Why `0x400`?  To store the 8 most significant mantissa bits, they need to be rounded first.
Adding this (TODO) guarantees (TODO) they are correctly rounded upwards within 10 (TODO 8?) bits:

~~~
  0eee eeee eeee mmmm mmmm rrrr rrrr rrrr
+ 0000 0000 0000 0000 0000 0100 0000 0000
  ---------------------------------------
  0eee eeee eeee mmmm mmmm
~~~

TODO

Now, since x87 uses 80-bit floating point internally, and the LUT only stores
the 8 most significant bits of the mantissa, this could easily be recompiled for SSE or something newer.  The
extra precision isn't used.

So now the LUT stores the rounded most significant 8 bits of the `float64` mantissa for `1.0/sqrt(x)`.

In the actual function, we can get the correct 8 bits... if we know which table entry to use.

First, lets assume our original `[0.5 .. 2)` input as it was.  Now we can immediately see that we can simply
index by ... TODO ... to obtain the 8 most significant mantissa bits of the result.

So this is interesting.  The LUT contains TODO 256 entrites and the bottom 128 entries are used in the ODD (TODO) case.
the top X entries are used in the even case.


The odd entries have this form:

{{< katex display >}}
\mathrm{LUT}_{\mathrm{odd}}[i] = \frac{1}{\sqrt{\frac{1 + \frac{i}{128}}{2}}}
{{< /katex >}}

The even entries have this form:

{{< katex display >}}
\mathrm{LUT}_{\mathrm{even}}[i] = 2*\frac{1}{\sqrt{1 + \frac{i}{128}}}
{{< /katex >}}

Putting the two together:
{{< katex display >}}
\mathrm{LUT}[i] = \begin{cases} \mathrm{LUT}_{\mathrm{odd}}[i],\ \mathrm{if}\ i < 128 \\
  \mathrm{LUT}_{\mathrm{even}}[i - 128],\ \mathrm{otherwise} \end{cases}
{{< /katex >}}

Recall the mantissa shift... TODO...

## The fast reciprocal square root function `i76_rsqrt`

Recall what our decompiled `i76_rsqrt` function looks like:

{{< highlight cpp >}}
float10 i76_rsqrt(double param_1)
{
  uint uVar1;
  float10 fVar2;
  
  uVar1 = (0xbfc - (param_1._4_4_ >> 0x14) >> 1) << 0x14 |
          (uint)global_lut[param_1._4_4_ >> 0xd & 0xff] << 0xc;
  fVar2 = (float10)(double)((ulonglong)uVar1 << 0x20);
  return ((float10)double_3.0 - fVar2 * fVar2 * (float10)param_1) *
         (float10)(double)((ulonglong)uVar1 << 0x20) * (float10)double_0.5 *
         (float10)double_approx_1.00001;
}
{{< /highlight >}}

Now we'll take this function apart piece by piece to figure out what the magic is.  The general algorithm is as follows:

1. Form an initial guess
  * Get the mantissa bits from the LUT
  * Solve for the exponent bits using a formula
2. Perform 1 iteration of Newton-Raphson on the guess
3. Apply a fixup multiplying the result by 1.0001

We'll address each section in order.  

## Forming the initial guess

The initial guess is also a `float64`.  Since it has three components (sign, biased exponent, and mantissa),
wouldn't it be nice if one could solve for each of them independently?
As it turns out, it is possible do just that!

### Sign bit

First, observe that the sign bit of the guess has to be 0, which denotes a non-negative number in IEEE-754.
This is reciprocal square root is always a positive value.  So that one was easy! :)

#### Guess status

~~~
0    ??????????? ????????????????????????????????????????????????????
^    ^           ^
sign exponent    mantissa
~~~

Hey, it's a start!

### Exponent

Now onto solving for the exponent field.

{{< highlight cpp "hl_lines=6 8" >}}
float10 i76_rsqrt(double param_1)
{
  uint uVar1;
  float10 fVar2;
  
  uVar1 = (0xbfc - (param_1._4_4_ >> 0x14) >> 1) << 0x14 |
          (uint)global_lut[param_1._4_4_ >> 0xd & 0xff] << 0xc;
  fVar2 = (float10)(double)((ulonglong)uVar1 << 0x20);
  return ((float10)double_3.0 - fVar2 * fVar2 * (float10)param_1) *
         (float10)(double)((ulonglong)uVar1 << 0x20) * (float10)double_0.5 *
         (float10)double_approx_1.00001;
}
{{< /highlight >}}

One way to figure out what the exponent should be in our `float64` is to [take the floor of the base-2 logarithm of the value](#ieee754-decomp), denoted $\lg$:

{{< katex display >}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\floor{\lg\left(\frac{1}{\sqrt{v}}\right)} \\
=\floor{\lg(1) - \lg(\sqrt{v})} \\
=\floor{0 - \frac{\lg(v)}{2}} \\
=\floor{-\frac{\lg(2^\fexp(1 + \ffrac))}{2}} \\
=\floor{-\frac{\lg(2^\fexp)}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}}
{{< /katex >}}

Observe that $1 \leq 1 + \ffrac < 2$.  Therefore:

{{< katex display >}}
0 \leq \lg(1 + \ffrac) < 1 \\
\implies 0 \leq \frac{\lg(1 + \ffrac)}{2} < \frac{1}{2}
{{< /katex >}}

Also observe that $\fexp/2$ may not be an integer -- some odd/even analysis is in order.

#### Odd case

Assume that $\fexp$ is odd:

{{< katex display >}}
\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=\floor{\floor{-\frac{\fexp}{2}} + \frac{1}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=\floor{-\frac{\fexp}{2}} \\
=-\frac{\fexp}{2} - \frac{1}{2} \\
=-\frac{\fexp + 1}{2}
{{< /katex >}}

Note that $\fexp + 1$ is even, and therefore $-\frac{\fexp + 1}{2}$ is a valid exponent for IEEE-754.

#### Even cases

Now assume that $\fexp$ is even.  Therefore $\fexp/2$ is indeed an integer.
However, there are two cases to deal with: $\ffrac = 0$ and $\ffrac \neq 0$.  If $\ffrac = 0$ then:

{{< katex display >}}
\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=-\frac{\fexp}{2}
{{< /katex >}}

On the other hand, if $\ffrac \neq 0$ then :

{{< katex display >}}
\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=-\frac{\fexp + 2}{2}
{{< /katex >}}

### Putting it together

Let's recap.  We have three cases:
1. $\fexp$ is odd $\implies r_\fexp = -\frac{\fexp + 1}{2}$  
2. $\fexp$ is even and $\ffrac = 0 \implies r_\fexp = -\frac{\fexp}{2}$
3. $\fexp$ is even and $\ffrac \neq 0 \implies r_\fexp = -\frac{\fexp + 2}{2}$

Now we can directly determine what the exponent should be for the result.  What is especially cool
about this is that no square roots or divisions are needed to do this -- just an addition and a right shift!

### Biasing the exponent 

But we still need to bias the exponent!  
What would the biased exponent be?

1. $rexp_1 = -(\fexp + 1)/2$
2. $rexp_2 = -(\fexp/2)$
3. $rexp_3 = -(\fexp + 2)/2$

So the actual biased value would be:

1. $bexp_1 = rexp_1 + 1023 = -(\fexp + 1)/2 + 1023 = (2046 - \fexp - 1)/2 = (2045 - \fexp)/2$
2. $bexp_2 = rexp_2 + 1023 = -(\fexp/2) + 1023 = -(\fexp/2) + 2046/2 = (2046 - \fexp)/2$
3. $bexp_3 = rexp_3 + 1023 = -(\fexp/2 - 1) + 1023 = -(\fexp/2 - 1) + 2046/2 = (2044 - \fexp)/2$

And, notice we can take in a biased exponent as well, leading us to:

...

That looks a *lot* like our `0xBFC` constant, doesn't it?  

To keep it simple, they always selected the *odd* case, probably
probabilistically: it covers half of the possible inputs,
while the *even* cases split the remaining half depending on if $\ffrac = 0$.

Remember that branching
was likely expensive back in 1997 and before, as [out of order execution](https://en.wikipedia.org/wiki/Out-of-order_execution)
and its cousin [speculative execution](https://en.wikipedia.org/wiki/Speculative_execution) were not really a thing on x86 quite yet.
However, [superscalar]() execution definitely *was* a thing, since x87 coprocessor ran in parallel
with the the x86 core.  Since the pipeline was in-order, branching might have incurred a performance penalty
(a [pipeline stall](https://en.wikipedia.org/wiki/Pipeline_stall)) as execution could not proceed until the branch was resolved
And besides, this is just supposed to be an
initial guess anyway -- it doesn't need to be perfect.  If you had to hard-code a path to take, then selecting the most
probable case made sense.

Also note that TODO this means the actual result will be at most a factor of $\sqrt{2}$ off.
This is acceptable given that this is supposed to be just a first guess... TODO

#### Guess status

~~~
0    eeeeeeeeeee ????????????????????????????????????????????????????
^    ^           ^
sign exponent    mantissa
~~~

Making progress!  Just one more field to go.

## Mantissa

Onto solving for the mantissa field now.

{{< highlight cpp "hl_lines=7 8" >}}
float10 i76_rsqrt(double param_1)
{
  uint uVar1;
  float10 fVar2;
  
  uVar1 = (0xbfc - (param_1._4_4_ >> 0x14) >> 1) << 0x14 |
          (uint)global_lut[param_1._4_4_ >> 0xd & 0xff] << 0xc;
  fVar2 = (float10)(double)((ulonglong)uVar1 << 0x20);
  return ((float10)double_3.0 - fVar2 * fVar2 * (float10)param_1) *
         (float10)(double)((ulonglong)uVar1 << 0x20) * (float10)double_0.5 *
         (float10)double_approx_1.00001;
}
{{< /highlight >}}

Let's take what we learned about the exponent and imagine if we took the reciprocal square root of $v$.
What would it look like?

{{< katex display >}}
\frac{1}{\sqrt{v}} \\
= \frac{1}{\sqrt{2^\fexp(1 + \ffrac)}} \\
= \frac{1}{\sqrt{2^\fexp}\sqrt{1 + \ffrac}} \\
= 2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}}
{{< /katex >}}

Now this is interesting.
First, observe the following about: $\frac{1}{\sqrt{1 + \ffrac}}$:

{{< katex aligned >}}
&0 \leq \ffrac < 1 \\
\iff& 1 \leq 1 + \ffrac < 2 \\
\iff& 1 \leq \sqrt{1 + \ffrac} < \sqrt{2} \\
\iff& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} \leq 1 \tag{1}
{{< /katex >}}

TODO: SSO style, _odd, _even analysis, bring it together at the very end?

So by itself, this is not a suitable mantissa.  However, we know the exponent isn't quite right either, according
to the cases we've explored in the previous section.  Let's fix that and break this out on the cases outlined previously:

### $\fexp$ is odd

In this case, the floating point exponent of the guess is $-\frac{\fexp + 1}{2}$.  Let's work backwards to get there:

{{< katex aligned >}}
&2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1/2 + 1/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1/2}\frac{\sqrt{2}}{\sqrt{1 + \ffrac}} \\
=&2^{-(\fexp + 1)/2}\frac{1}{\frac{\sqrt{1 + \ffrac}}{\sqrt{2}}} \\
=&2^{-(\fexp + 1)/2}\frac{1}{\sqrt{\frac{1 + \ffrac}{2}}}
{{< /katex >}}

Note that, from $(1)$

{{< katex aligned >}}
& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} \leq 1 \\
\iff& 1 < \frac{\sqrt{2}}{\sqrt{1 + \ffrac}} \leq \sqrt{2}  \\
\implies& 1 \leq \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} < 2
{{< /katex >}}

This we can work with.  If we choose:

{{< katex display >}}
m = \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} - 1
{{< /katex >}}

Since $0 < m < 1$ and $\fexp + 1$ is even, then:

{{< katex display >}}
 2^{-(\fexp + 1)/2}(1+m)
{{< /katex >}}

is a valid IEEE-754 floating point value!  We have solved the problem for the odd case.
Next we will address the even cases.

### $\fexp$ is even and $\ffrac = 0$

Here, we know the floating point exponent of the guess is $-\frac{\fexp}{2}$.  Since $\ffrac = 0$, we already
know the mantissa of the result will be $1$:

{{< katex aligned >}}
&2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2}\frac{1}{\sqrt{1 + 0}} \\
=&2^{-\fexp/2}
{{< /katex >}}

That was easy!  Therefore, if we let $m = 0$, then

{{< katex display >}}
 2^{-(\fexp + 1)/2}(1+m)
{{< /katex >}}

is our IEEE-754 floating point result.

### $\fexp$ is even and $\ffrac \neq 0$

This is the hardest case to work with.  First, we know our exponent is $-\frac{\fexp + 2}{2}$

Let's work backwards once again:

{{< katex aligned >}}
&2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1 + 1}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1}\frac{2}{\sqrt{1 + \ffrac}} \\
{{< /katex >}}

Now here, $\ffrac \neq 0$, so $(1)$ simplifies to:

{{< katex display >}}
\frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} < 1
{{< /katex >}}

This is important as it allows the following:

{{< katex aligned >}}
& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} < 1 \\
\iff& \sqrt{2} < \frac{2}{\sqrt{1 + \ffrac}} < 2 \\
\implies& 1 \leq \frac{2}{\sqrt{1 + \ffrac}} < 2 
{{< /katex >}}

Therefore, if we set

{{< katex display >}}
m = \frac{2}{\sqrt{1 + \ffrac}} - 1
{{< /katex >}}

$0 \leq m < 1$ and:

{{< katex aligned  >}}
 =& 2^{-(\fexp + 2)/2}(1 + m)
{{< /katex >}}

is a valid IEEE-754 floating point value!.

TODO: are these looking familiar?  Recall section...

### Recap

Let's recap.  We have three cases for choosing the mantissa: TODO
1. $\fexp$ is odd $\implies r_\ffrac = ...$
2. $\fexp$ is even and $\ffrac = 0 \implies r_\ffrac = ...$
3. $\fexp$ is even and $\ffrac \neq 0 \implies r_\ffrac = ...$

This is all well and good for the pure math version, but still, where does $r_\ffrac$ actually come from?  

### Approximating the mantissa

Let's revisit what we know about the LUT:

...

Indeed, we see the *odd* case selects the lower half of the LUT while the *even* case selects the upper half...

Within this, we see the LUT stores approximations of ... in the lower half and of ... in the upper half.

Since we're constructing an initial guess, this loss of information is acceptable.   We could state:

...

Now we're talking.

#### Guess status

~~~
0    eeeeeeeeeee mmmmmmmm00000000000000000000000000000000000000000000

sign exponent    mantissa
~~~

We now have enough information to construct our first guess!

### LUT fixup

{{< highlight cpp "hl_lines=14" >}}
void generate_lut(void)
{
  int uVar1;
  double local_10;
  double var1;
  
  uVar1 = 0;
  do {
    var1._4_4_ = (int)((ulonglong)
      (1.0 / SQRT((double)((ulonglong)((uVar1 | 0x1ff00U) << 0xd) << 0x20))) >> 0x20);
    global_lut[uVar1] = (byte)(var1._4_4_ + 0x400 >> 0xc);
    uVar1 = uVar1 + 1;
  } while (uVar1 < 0x100);
  global_lut[128] = 0xff;
  return;
}
{{< /highlight >}}

"Now that's all well and good", you say.  "But what happens when the value of 1.0 is passed in as input?"

Ah, yes, we have a bit of a problem here, don't we? `1.0` would have an exponent of 0 associated with it,
which is an even quantity.  Running it yields:

...

Well, that's not good.  Our LUT is correct, but we're choosing a lower exponent
than we should be due to forcing the *odd* case for exponent selection.
How can we fix that without branching?  Easy!  We just patch the LUT specifically
for that case:

...

Note the `0xFF` here -- this means we're setting the 8 most significant mantissa bits
to all ones.  Since the power is set to `-1` due to the *odd* rule working,
this means our true exponent is `0.5` -- so to try to get a value closer to 1, we 
set the mantissa bits to ones to compensate, bringing us closer:

...


TODO: picture here

Now our initial guess for `1.0` is much closer.  Indeed, this would affect anything...

TODO: CODE SNIPPET

## Newton-Raphson iteration

{{< highlight cpp "hl_lines=9 10" >}}
float10 i76_rsqrt(double param_1)
{
  uint uVar1;
  float10 fVar2;
  
  uVar1 = (0xbfc - (param_1._4_4_ >> 0x14) >> 1) << 0x14 |
          (uint)global_lut[param_1._4_4_ >> 0xd & 0xff] << 0xc;
  fVar2 = (float10)(double)((ulonglong)uVar1 << 0x20);
  return ((float10)double_3.0 - fVar2 * fVar2 * (float10)param_1) *
         (float10)(double)((ulonglong)uVar1 << 0x20) * (float10)double_0.5 *
         (float10)double_approx_1.00001;
}
{{< /highlight >}}

Next, one iteration of [Newton-Raphson](https://en.wikipedia.org/wiki/Newton%27s_method) is performed using the initial guess as previously computed.
This is exactly the same as what appears in the Quake 3 `Q_rsqrt` function.  The formula derivation is as follows.

{{< katex display >}}
y = \frac{1}{\sqrt{v}}
\iff y^2 = \frac{1}{v}
\iff vy^2 = 1
\iff \frac{1}{y^2} = v
\iff \frac{1}{y^2} - v = 0
{{< /katex >}}

Let $f(y) = \frac{1}{y^2} - v$.  If we can solve for $f(y) = 0$ for a given $v$ then $y$ is the reciprocal square root of $v$.  The derivative of $f$ with respect to $y$ is:

{{< katex display >}}
f'(y) = -2y^{-3}
{{< /katex >}}

One iteration of Newton-Raphson is evaluated as follows:

{{< katex display >}}
y_1 = y_0 - \frac{f(y_0)}{f'(y_0)}
{{< /katex >}}

This simplifies to:

{{< katex display >}}
y_1 = \frac{(3 - y_0^2v)y_0}{2}
{{< /katex >}}

We evaluate $y_1$ by setting $y_0$ to the initial guess as computed earlier.

Looking at our decompiled code, we can see a direct implementation of this formula, but with with a strange extra multiplication of `1.00001` at the end...

## Post-hoc fixup

{{< highlight cpp "hl_lines=11" >}}
float10 i76_rsqrt(double param_1)
{
  uint uVar1;
  float10 fVar2;
  
  uVar1 = (0xbfc - (param_1._4_4_ >> 0x14) >> 1) << 0x14 |
          (uint)global_lut[param_1._4_4_ >> 0xd & 0xff] << 0xc;
  fVar2 = (float10)(double)((ulonglong)uVar1 << 0x20);
  return ((float10)double_3.0 - fVar2 * fVar2 * (float10)param_1) *
         (float10)(double)((ulonglong)uVar1 << 0x20) * (float10)double_0.5 *
         (float10)double_approx_1.00001;
}
{{< /highlight >}}

Let's go back and try our `1.0` example again:

So we're pretty close with the LUT fixup, but notice we're still at `0.9999`.  This isn't a dealbreaker, but it *is* annoying.
How can we fix that?  Well, one way is to just apply a post-hoc fixup -- say, by scaling it up so it fits
better.  It only needs to be accurate to within 4 decimal places, so what do we do?  Multiply it by `1.00001`!

...

That's better.  Calling the function with `1.0` now gives you something that looks about right.  Note that this
could also have been achieved by addition as well.  I'm not sure what the tradeoff is, exactly.  As both x87 `fmul` and `fadd`
seem to have the same cost from what I can tell, I can only assume this choice was deliberate for some non-performance reason.

## The big picture

Now we have a complete picture of how the Mechwarrior 2 engine (as it exists in I'76) evaluates the reciprocal square root.

LUT generation:
1. Cache $1/\sqrt{v}$ for v in $[0.5 .. 2)$ 
2. Apply post-hoc fixup for LUT[0x80]

Reciprocal square root function:
1. Get the mantissa bits from the LUT (adjusted) indexed by... (prove by theorem by index) TODO
2. Solve for the exponent bits using the formula (theorem) TODO
3. Combine these to form an initial guess
4. Perform 1 iteration of Newton-Raphson
5. Apply a post-hoc fixup multiplying the result by 1.0001 (to make 1.0 map to 1.0)

## Cleaned up rsqrt

Putting all this together, I present a much cleaned up C++20 version of the fast reciprocal square root code:

{{< highlight cpp >}}
double i76_rsqrt(double const scalar)
{
    uint64_t const scalar_bits = std::bit_cast<uint64_t>(scalar);

    uint8_t const index = (scalar_bits >> 0x2d) & 0xff;

    //LUT[index] is our mantissa, rounded up, all lower 34 bits zeroed out
    uint64_t const mantissa_bits = static_cast<uint64_t>(LUT[index]) << 0x2c;

    uint64_t const exponent_bits = ((0xbfcU - (scalar_bits >> 0x34)) >> 1) << 0x34;
    
    //exponent_bits has form 0xYYY00000 00000000
    //mantissa_bits has form 0x000ZZ000 00000000
    //so combined, we have   0xYYYZZ000 00000000 -- a complete float64 for the guess
    uint64_t const combined_bits = exponent_bits | mantissa_bits;

    auto const initial_guess = std::bit_cast<double>(combined_bits);

    auto const half_initial_guess = initial_guess * 0.5;
    auto const initial_guess_squared = initial_guess * initial_guess;

    auto const newton_raphson = (3.0 - (scalar * initial_guess_squared)) * half_initial_guess;
    auto const fixup = newton_raphson * 1.00001;

    return fixup; 
}
{{< /highlight >}}

Here's a comparison with the original in the same vein as the LUT generation code:

{{< highlight nasm >}}
; original I'76 rsqrt               | ; new code, clang 11 -Ofast -mno-sse -std=c++2b -m32 
                                      sub     esp, 014h
mov ecx,dword ptr ss:[esp+8]        | fld     qword ptr [esp + 018h] 
mov edx,BFC                         | fst     qword ptr [esp + 8] 
shr ecx,14                          | mov     eax, dword ptr [esp + 0Ch] 
mov eax,dword ptr ss:[esp+8]        | mov     ecx, eax 
shr eax,D                           | shr     ecx, 0Dh 
sub edx,ecx                         | movzx   ecx, cl 
shr edx,1                           | movzx   ecx, byte ptr [ecx + LUT] 
and eax,FF                          | shl     ecx, 0Ch 
fld st(0),qword ptr ss:[esp+4]      | shr     eax 
shl edx,14                          | and     eax, 07FF80000h
mov al,byte ptr ds:[eax+5FB180]     | mov     edx, 05FE00000h
shl eax,C                           | sub     edx, eax 
xor ecx,ecx                         | and     edx, 0FFF00000h
or edx,eax                          | or      edx, ecx 
mov dword ptr ss:[esp+4],ecx        | mov     dword ptr [esp + 4], edx 
mov dword ptr ss:[esp+8],edx        | mov     dword ptr [esp], 0 
fld st(0),qword ptr ss:[esp+4]      | fld     qword ptr [esp] 
fmul st(0),st(0)                    | fld     st(0) 
fld st(0),qword ptr ss:[esp+4]      | fmul    st, st(1) 
fxch st(0),st(1)                    | fmulp   st(2), st 
fmulp st(2),st(0)                   | fxch    st(1) 
fmul st(0),qword ptr ds:[501168]    | fsubr   dword ptr [__real@40400000 (08CF000h)] 
fxch st(0),st(1)                    | fxch    st(1) 
fsubr st(0),qword ptr ds:[501160]   | fmul    qword ptr [__real@3fe0000a7c5ac472 (08CF008h)] 
fmulp st(1),st(0)                   | fmulp   st(1), st 
fmul st(0),qword ptr ds:[501170]    | 
                                    | add     esp, 014h
ret                                 | ret
{{< /highlight >}}

The code is far more readable *and* it seems to be about on par with the original!
The main difference here is that Clang [reordered](#fp-assoc) the floating point operations (due to fast math optimizations)
and didn't clobber the argument passed on the stack, instead doing scratch work in newly
allocated memory.  It does seem that it could do a better job if it wanted, but I'm not complaining.

## Analysis

Numerically, we know our initial guess will be at least a factor of $\sqrt{2}$ off... TODO

Running the code... vectorizes very nicely...

Associativity...

Note that the method presented here could be extended to increase the guessing accuracy, but each extra bit doubles the size of the LUT!

## Patching the game

Now, let's put our theory into practice: let us replace this function in I'76 with an upgraded version that is both
faster and more accurate using `SSE2` (3? 4? TODO).

### Screenshots/video? TODO

# Conclusion

Well that was fun, wasn't it?

I suspect that a fast reciprocal square root function would have been a big deal in 1997, and especially earlier if it was present in the original
Mechwarrior 2.  Being able to quickly normalize vectors would have made 3D graphics much easier to do
on the CPU, even if the results weren't perfect.  Although methods have existed to do this since the mid eighties, it seems they weren't
well known outside of a few circles.

We're really lucky these days in that tricks like these are very often not even needed anymore.
Nowadays, you can just write `1.0/sqrt(x)` and expect good performance 99% of the time.
This code was written for a time when processors were a lot weaker, and you *needed* to be clever to get
things working at an acceptable speed. Bit-twiddling was the norm to get things done.
Now, fast reciprocal square root is a hardware feature of many processors out there,
so tricks like this are rarely even needed.  
*In other words, you probably don't want to use this in new code*!  But if you *do* need something like this,
C++20 makes it easier than it has ever been to write readable bit twiddling code.

Also, just a gentle reminder to always profile your code before attempting to optimize anything.
Do measurements, do test runs, etc.  You'll thank yourself later!  If you don't do this,
you might actually end up *hurting* your own performance by pre-emptively implementing something
like `i76_rsqrt` in new code.
Use a good profiler like [vTune](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/vtune-profiler.html) to help you understand what parts
of your code are slow, and most importantly, *why*.  There are so, so many things it could be, and chances
are you won't be able to figure it out by just looking at the code itself.  *World experts* have a hard time figuring it out.
Save yourself the trouble!  Write your code with clarity first, and profile afterwards.  You might be surprised at what you see.

The code demonstrated in this article was designed to be clear from the get-go, to show how well clean code optimizes in C++20.
Even in 32-bit mode, MSVC optimizes the 64-bit values right out and basically transformed it into the original code,
and it's *way* easier to read.  If a fellow human can understand your code, you bet the compiler can too!

For fun, you'll notice the LUT generation actually auto-vectorizes very well too -- not
that it matters anyway, because it could have just been `constexpr` in the first place :) I expect the actual `rsqrt`
function would vectorize well enough too.

I've put the code on [Compiler Explorer](TODO) (ALSO TL;DR) in case anyone wants to play around with it.

Do note that building on different compilers could give [slightly different results](#fp-assoc) if fast math is enabled.
The code is available on [GitHub](TODO) as well as a CMake project.

Thanks for tuning in!  This article took me way too long to write, but I'm sure glad I got it out of my system.

Next up in my I'76 hacking: finding an old bug where ramming your car in Nitro would cause your health to reset (I suspect it's an overflow bug).

# Appendix

## IEEE-754 special cases{#ieee754-cases}

Special cases of IEEE-754 binary datatypes:

* Zero is represented by zeroing out all bits in the datatype
* `Inf` is represented by setting all exponent bits to $1$ and all mantissa bits to $0$
  * The sign bit determines whether it is `+Inf` or `-Inf`
* [NaN](https://en.wikipedia.org/wiki/NaN) (*Not-a-Number*) is represented by setting all exponent bits to $1$
  * The sign bit determines whether the `NaN` is signaling or quiet
  * At least one mantissa bit is set to $0$
* [Subnormal numbers](https://en.wikipedia.org/wiki/Denormal_number) have all exponent bits set to $0$ with at least one mantissa bit set to $1$

## IEEE-754 float64 decomposition{#ieee754-decomp}

Let $x \in \mathbb{R}, x \geq 0$.  Then:

{{< katex display >}}
x = 2^\floor{\lg\left(x\right)}*\frac{x}{2^\floor{\lg\left(x\right)}}
{{< /katex >}}

This works because:

{{< katex aligned >}}
& 0 \leq \lg\left(x\right) - \floor{\lg\left(x\right)} < 1 \\
\implies& 2^0 \leq 2^{\lg\left(x\right) - \floor{\lg\left(x\right)}} < 2^1 \\
\implies& 1 \leq \frac{2^{\lg\left(x\right)}}{2^\floor{\lg\left(x\right)}} < 2 \\
\implies& 1 \leq \frac{x}{2^\floor{\lg\left(x\right)}} < 2
{{< /katex >}}

To represent this as a `float64`, two conditions need to be met:

* $-1023 < \floor{\lg\left(x\right)} < 1024$, otherwise this is an overflow or underflow condition (possibly requiring a subnormal)
* $\frac{x}{2^\floor{\lg\left(x\right)}} - 1$ can fit in the mantissa (52 bits), otherwise rounding is needed

Handling negative values is simple as well -- treat it as a positive value and then set the sign bit accordingly.

## Floating point associativity{#fp-assoc}

You may notice that if you attempt to build the code using fast math, you get results that are not identical across the MSVC and Clang compilers.
This is due to the compiler reordering the floating point operations -- something it would ordinarily not be permitted to do without `-Ofast` or equivalent.

For example, on Clang 11, the compiler chooses to evaluate this code in `i76_rsqrt`:

{{< highlight cpp >}}
auto const half_initial_guess = initial_guess * 0.5;
auto const initial_guess_squared = initial_guess * initial_guess;

auto const newton_raphson = (3.0 - (scalar * initial_guess_squared))
                          * half_initial_guess;
auto const fixup = newton_raphson * 1.00001;

return fixup;
{{< /highlight  >}}

as:

{{< highlight cpp >}}
return ((3.0 - (scalar * initial_guess_squared)) * (initial_guess * (0.5 * 1.00001)));
{{< /highlight  >}}

Note the shifting of the parentheses: `(initial_guess * 0.5) * 1.00001` is changed to `initial_guess * (0.5 * 1.00001)`.
In pure math, these expressions are equivalent because addition and multiplication on the reals are associative.
That is, the following two statements hold for all $a, b, c \in \mathbb{R}$:

{{< katex display >}}
(a + b) + c = a + (b + c)
{{< /katex >}}
{{< katex display >}}
a(bc) = (ab)c
{{< /katex >}}

However this is not the case for floating point operations due to the rounding that occurs after each operation.  Since floating point types have a fixed size,
 the result of each operation needs to be rounded to fit within the datatype.  There are different [rounding modes](https://en.wikipedia.org/wiki/IEEE_754#Rounding_rules) available to achieve this.
Since floating point math isn't associative in general, the optimization above actually doesn't preserve the original program semantics.
However, it *is* faster -- `0.5 * 1.00001` is evaluated at compile time, saving one multiplication at run-time.

Does it matter?  Well, it depends on the code, really.  For an approximation like `i76_rsqrt`, it's well within the error
and doesn't matter one bit, and these applications are precisely why compilers offer fast math options.
For algorithms that really depend on having things done in a certain order to minimize [roundoff error](https://en.wikipedia.org/wiki/Round-off_error)?
Then you bet it matters a great deal!  This is why fast math optimizations are not enabled by default in most compilers.

### x87 refresher{#fp-x87}

Back in 1997, things like SSE and AVX didn't quite exist yet on x86.  I think MMX might have
just recently been released, but it wasn't something you could rely on being there.  What you most likely had
was the x87 instruction set to work with, and this thing came with a number of peculiarities.
Originally an optional co-processor on earlier Intel chipsets, it was integrated into the CPU
proper with the advent of the i487.  The co-processor nature of it is reflected in the ISA itself.
It has it's own floating point stack, and interacting with it involves pushing, reordering, and popping things from that stack.

It also curiously has it's own 80-bit floating point representation internally, which seems
to essentially be `float64` with extra mantissa bits and an explicit normalization bit.