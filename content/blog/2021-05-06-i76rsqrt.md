+++
title = "How Activision implemented a fast reciprocal square root function in 1996"
date = "2021-05-06"
tags = ["i76"]
draft = true
+++

This article is part of my series on reverse engineering [Interstate '76](https://en.wikipedia.org/wiki/Interstate_%2776), with my
current goal being to add a [Vulkan](https://en.wikipedia.org/wiki/Vulkan_(API)) renderer to the game.

# Foreword

You know, back when I first started delving into reverse engineering I'76 ten years ago,
tools like [Ghidra](https://ghidra-sre.org/) weren't available.  The state of the art
was probably [IDA Pro](https://www.hex-rays.com/ida-pro/), a tool suite that while reasonably 
priced for what it did, was way out of the price range for me, a third year undergraduate
student.  Sure, pirating it was an option, but if you wanted to do things the honest way *and* on a budget, it was
hard to do better than [Ollydbg](http://www.ollydbg.de/version2.html), a free debugger
and disassembler for 32-bit x86.  Now, ten years later, the landscape looks radically
different.  There are so many fantastic tools available for doing this kind of thing,
and it has never been easier to mod your favourite games as a result.

# Introduction

Everyone is familiar with the famous [fast reciprocal square root function in the Quake
3 source code](https://en.wikipedia.org/wiki/Fast_inverse_square_root).  And, as noted
on Wikipedia, solutions have existed for computing the fast reciprocal square root
for many years before that, with perhaps the earliest implementation in 1986.

While I was working on reverse engineering Interstate '76, I discovered that Activision
had their own implementation in use from at least 1997, and it shares some similarities
with the Quake 3 approach.  If I had a copy of Mechwarrior 2 (DOS version) on hand, I could confirm
if this was present there as well, which would date this technique even
earlier.  In this post, I'll explain how it works.

# The TL;DR

The approach used in the Mechwarrior 2 engine computes an approximation of the reciprocal
square root using the x87 instruction set at `float64` (or `double`) precision.  At initialization
time, the engine generates a Lookup Table ([LUT](https://en.wikipedia.org/wiki/Lookup_table))
containing the 8 most significant mantissa bits of the reciprocal square root.  This LUT
has only 256 entries and covers the range TODO to TODO.  It is manually adjusted to
create more desirable characteristics around the value `1.0` (TODO).

TODO: INSERT PICTURE HERE

Later, the actual reciprocal square root function uses this LUT to help form
an initial guess for the result.

## Assumptions about the input

As we know, an IEEE-754 `float64` is represented using a
sign, biased exponent, and mantissa.  Because we're dealing with approximations,
it's fair to make some assumptions about our input.  We assume the input is:

* non-`NaN`
* non-`InF` (i.e. finite)
* normal (i.e. not denormal/subnormal)
* positive (i.e., the sign bit is zero)
 
These assumptions simplify the math
and handle the vast majority of use cases that this function would be used for.  Remember:
this thing is supposed to be fast and reasonably accurate, not perfect.

## Forming the initial guess

The initial guess is also a `float64`.  Since it has three components (sign, biased exponent, and mantissa),
wouldn't it be nice if one could solve for each of them independently?
As it turns out, it is possible do just that!

### Sign bit

First, observe that the sign bit of the guess has to be 0, which denotes a non-negative number in IEEE-754.
This is reciprocal square root is always a positive value.  So that one was easy!  But we still
have the exponent and mantissa to worry about.

TODO: PICTURE HERE

### Biased exponent

IEEE-754 stores the exponent of the number using a biasing value (TODO).
What this effectively means is that TODO is added to the actual exponent before being
stored in the `float64`.

In math, we can represent a `float64` conforming to our assumptions as follows:

TODO: FORMULA HERE

Now, imagine if we took the inverse square root of this value.  What would it look like?
As it turns out, a clever formula can be used to directly compute what the
exponent bits of the guess should be directly from the input!

TODO: DERIVATION HERE

Note that there are three cases:

* The exponent is odd
* The exponent is even and ...
* The exponent is even and not ...

To keep it simple, Activision always selected the first case, probably
probabilistically: the *odd* case covers half of the possible inputs,
while the *even* cases split the remaining halves.  Remember that branching
was likely expensive back in 1997 and before, as [out of order execution]()
and its cousin [speculative execution]() were not really a thing on x86 quite yet.
However, [superscalar]() execution definitely *was* a thing, since x87 coprocessor ran in parallel
with the the x86 core.  Since the pipeline was in-order, branching might have incurred a performance penalty
(a phenomenon TODO bubbling the pipeline).  And besides, this is just supposed to be an
initial guess anyway -- it doesn't need to be perfect.  If you had to hard-code a path to take, then selecting the most
probable case made sense.

"Now that's all well and good", you say.  "But what happens when the value of 1.0 is passed in as input?"

TODO: power vs exponent

Ah, yes, we have a bit of a problem here, don't we? `1.0` would have an exponent of 0 associated with it,
which is an even quantity.  Running it yields:

...

TODO: discuss mantissa recovery before this section

Well, that's not good.  Our LUT is correct, but we're choosing a lower exponent
than we should be due to forcing the *odd* case for exponent selection.
How can we fix that without branching?  Easy!  We just patch the LUT specifically
for that case:

...

Note the `0xFF` here -- this means we're setting the 8 most significant mantissa bits
to all ones.  Since the power is set to `-1` due to the *odd* rule working,
this means our true exponent is `0.5` -- so to try to get a value closer to 1, we 
set the mantissa bits to ones to compensate, bringing us closer:

...


TODO: picture here

### Mantissa bits

Next, the LUT is used to get the mantissa bits of the result...


### The big picture

Now we have a complete picture of how to form a guess:

1. Get the mantissa bits from the LUT (adjusted) indexed by... (prove by theorem by index)
2. Solve for the exponent bits using the formula (theorem)
3. Combine these to form an initial guess

## Newton-Raphson iteration

Next, one iteration of Newton-Raphson is performed on the initial guess.  The derivation of this is as follows:

...

Looking at our pseudocode, we can see directly this being implemented with a strange extra multiplication of `1.00001`...

## Post-hoc fixup

Let's go back and try our `1.0` example again:

So we're pretty close, but notice we're still at `0.9999`.  This isn't a dealbreaker, but it *is* annoying.
How can we fix that?  Well, one way is to just apply a post-hoc fixup -- say, by scaling it up so it fits
better.  It only needs to be accurate to within 4 decimal places, so what do we do?  Multiply it by `1.00001`!

...

That's better.  Calling the function with `1.0` now gives you something that looks about right.


## Analysis

I tried out the decompiled version of the code by sampling inputs and comparing them with the
correctly rounded "ground truth" result.  The code appears to give about 5 significant digits of precision
in the worst case, often providing 6 or more.  Not bad at all!

### Similarities with the Quake 3 code

The approach is almost exactly the same to the Quake 3 approach except that instead of
hard-coding an initial guess, the guess is obtained through a precomputed lookup table with 256 entries.
The lookup table stores the most significant 8 mantissa bits of the reciprocal square root for the `float64` values with
hex representation of the upper 32-bits being `0x3FE00000` through ... (with the remaining lower 32 mantissa bits zeroed out).
This corresponds to the range `[0.5..2)`, `[0.5..1)` having a spacing of ... and `[1, 2)` having a spacing of ...

The initial guess is already decent, but it is further
refined using one iteration of the [Newton-Raphson method](https://en.wikipedia.org/wiki/Newton%27s_method)
to get a closer result, accurate to about 4 decimal places in the absolute worst case.
Two post-hoc "fixups" are present: one in the generation of the lookup table, and one
after the iteration of Newton-Raphson is completed.

The approach is almost exactly the same to the Quake 3 approach except that instead of
hard-coding an initial guess, the guess is obtained through a precomputed lookup table with 256 entries.
The lookup table stores the most significant 8 mantissa bits of the reciprocal square root for the `float64` values with
hex representation of the upper 32-bits being `0x3FE00000` through ... (with the remaining lower 32 mantissa bits zeroed out).
This corresponds to the range `[0.5..2)`, `[0.5..1)` having a spacing of ... and `[1, 2)` having a spacing of ...


# The long version

As part of my work on trying to add a Vulkan backend to I'76, I stumbled upon an interesting little
function in the initialization sequence of the game.  It appeared to be generating some kind of LUT
using the result of a a reciprocal square root:

...

Hmm.  Interesting.  Looks like it's only caching the mantissa bits.  But why only 8 of them?

After looking where else this was used, I found only one reference in this function:

...

Wow, what on earth?  Is this thing supposed to compute a reciprocal square root?  Let's see how this function is used:

...

That basically does it -- this thing is being called on things that look like [dot products](https://en.wikipedia.org/wiki/Dot_product), which means it's likely
being used as part of vector normalization.  Let's try to run this thing ourselves, shall we?

...

Unfortunately, Cutter's emulation capabilities seem to not show the x87 stack (or at least, I'm not aware of how to get that to show).
But this isn't malware we're dealing with, it's a game from 1996.  So, let's try it in x86dbg instead:

...

That's better.  Let's try a few values, shall we?

First, we'll set a breakpoint to let the game initialize and then run all the way until the LUT is finished generating.
Then, we'll manully modify EIP (or RIP), the instruction pointer, to point to the start of the function.  Now,
we can populate the stack with whatever we want.  Let's try `1.0`:

...

Cool!  Let's try 0.5 -- this should give us the square root of 2:

...

Not bad!  OK, let's try some other powers of 2:

...

Neat!  It actually seems to do a pretty decent job.  
Ghidra's decompiler was very helpful for providing a quick overview of what's going on,
but for the sake of my understanding, I rewrote it in C++:

~~~

~~~

Now we can actually test it out in bulk.  This compiles to almost exactly
the same instructions present in I'76 when built in 32-bit, only x87 allowed.

### Some basic analysis

I made a spreadsheet to see how this compares to the "precise" result...

4 digits in the worst case?  Not bad!  Most of the time, you get 5 or more.
**Interesting that the error seems to repeat itself too...**

TODO: LINK TO RAW DATA



But how's this thing actually work?

## The lookup table

Let's first figure out exactly what's going on with the LUT.  For a refresher, this is how
a `float64` is represented:

### float64 refresher

~~~
s        eeeeeeeeeee mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
sign      exponent              mantissa
(1 bit)   (11 bits)             (52 bits)
~~~

The sign bit is `0` if the float is positive, and `1` if it is negative.
The exponent bits represent a *biased exponent*, with the number 1023 added into it.
This means that `1.0` is represented as follows:

~~~
0 11111111111 000.... 0
~~~

Or, in hex notation, `0x3FF0000000000000`. `float64` is kind of nice because the sign
and exponent bits end on a nibble boundary, so ...

Unlike `float32`...

IEEE floating point supports some special values like `+Inf`, `-Inf`, and `NaN`.
`NaN` is actually an entire range of values...

There is also the special case of *subnormal* (or *denormal*) values to deal with...

x86 is little-endian, meaning the mantissa bits come first in memory, with the sign and exponent bits coming last:

TODO: colour code
~~~
seeeeeeeeeeemmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm

       7       6       5       4       3       2       1       0
(bytes in memory)
~~~

So, byte 0 would correspond to the least significant 8 bits of the mantissa.

To keep things simple, we will assume our input is non-`NaN`, non-`Inf`, and normalized.
We're dealing with an approximation of a function, after all -- it doesn't have to be perfect,
 it just has to be good enough for the cases it has to work with.

### x87 refresher

Now, back in 1996, things like SSE and AVX didn't quite exist yet.  I think MMX... might have
existed, but it wasn't something you could rely on being there.  What you most likely had
was the x87 instruction set to work with, and this thing came with a number of peculiarities.
Originally an optional co-processor on earlier Intel chipsets, it was integrated into the processor
proper in the ... this is reflected in the ISA itself.  It has it's own floating point stack,
and interacting with it involves pushing, reordering, and popping things from that stack.

It also curiously has it's own 80-bit floating point representation internally, which seems
to essentially be `float64` with extra mantissa bits and an explicit normalization bit.

### LUT generation

The LUT is generated by this...

TODO: IN BROWSER DEMO

... Note the "fixup" at the end.  We'll see what this is for later.

## The `fast_rsqrt` function

### The "what"

Combining exponent bits with mantissa bits to get the guess

#### Getting the correct exponent bits

TODO: interactive demo

#### Recovering the correct mantissa bits from the LUT

### The "how"

Let's run through a few cases.  First, let's assume that our input is in `[0.5, 2)`...
As we can see, the mantissa bits are correctly recovered from the LUT and the exponent is correctly
calculated.

### The "fixups"

Two fixups: 1.00001 and the LUT adjustment


# Other stuff

The astute reader will notice that the LUT could have been computed entirely at compile time, or even just stored as
an array initially.  I'm glad that Visual C++ 6.0 wasn't that smart, because it would have made this a fair bit harder to do!


# Junk from my notes to fill in where needed

This approach pre-dates the Quake 3 method by about 3 years...

My work builds on UCyborg's work.

I first discovered the code when identifying functions in the startup sequence of `nitro.exe`. Some fairly
normal initialization code, then suddently this function that populates a LUT using, seemingly, one byte from the result of a `float64` `1.0/sqrt(x)` appears.
Only one other function referenced it, and the code was almost incomprehensible at first glance.  Bit shifts, floats
being treated like integers, and other interesting things abound.  It *could* be what I was suspecting, but maybe
it was just a small piece of code used for something else.  Not wanting to jump to conclusions, I decided
to study it a bit more.  I figured a good place to start would be to see how this mysterious function was used.  I found
fragments of code like this all over the place:

...

"*Wow, that sure looks like a dot product*," I thought to myself.  If the function was being called
on the results of dot products, it's almost certain it's being used in vector normalization.  Indeed,
in the fragment of code above, you can see the vector components being multiplied by the result.
Could this really be an implementation of a fast reciprocal square root function similar to the Quake 3 approach
but in *1997*?  I had to try it!

Wanted to play with some new tools, I gave *x64dbg* a spin as it appears to be a more modern
version of Ollydbg.  I set a breakpoint for right after the function where the LUT gets generated,
and then immediately replaced the instruction pointer to the start of the function.
Seeing where the argument was passed in on the x87 stack, I figured why not try the value `2`:

...

Indeed, after running the function, `0.7071`... was sitting in `ST(0)`.  That's `1/sqrt(2)` accurate to about ... significant digits.
"*OK fine,*" I thought, "*let's try out some other values too, just to be sure.*" So, I tried out `100`, `4`, `64`, and `0.5`:

...

In each case, the result seemed to be accurate to within ... significant digits.

"*That's it.  I'm convinced!*", I said out loud, "*But what the hell is it actually doing?!*"

The next step, I figured, would be to decompile it to C++ to help analyze it a bit easier.
This would be a manual process guided by Ghidra's own decompilation, which *mostly* got it right.
I wanted to compare it against a correctly rounded reciprocal square root to sample what the error
was across the board.

First, the decompiled LUT generator:

...

If you compile this for 32-bit with MSVC 2019, x87 only, you will get nearly identical assembly to what is in I'76:

...

This is acceptable.  Comparing the LUT generated here with the one visible in x64dbg yielded identical results,
so I was confident I got this right.  Now we can actually pick this apart a bit.  

TODO: `float64` blah blah

First, the loop: the range `[0.5 .. 2)` is iterated 256 (TODO) times using this formula:

~~~
0 1111111111e mmmm mmm 0000 ...
~~~

What this means is half of the iterations are between `[0.5 .. 1)` and half are between `[1 .. 2)`:

* `[0.5 .. 1)` is iterated with an interval of TODO (when `e` is 0)
* `[1 .. 2)` is iterated with an interval of TODO (when `e` is 1)

`1.0/sqrt(x)` is evaluated with each input.  The lower 32 bits of the mantissa are completely thrown away,
The result is something that looks like a `float32`, but with more exponent bits.  This is nicer to work with
because the sign and exponent bits are 12 bits combined, meaning the mantissa is aligned on a nibble (4-bit) boundary.
This makes it very easy to understand when looking at the hexadecimal representation: 

~~~
0x3FE0 0000

  3FE
|    1 2345

sign bit: 0
exponent bits: 0x3FF (1023 biased) (0 unbiased)
mantissa bits: 0x12345
~~~

Each iteration of the loop populates one entry of the LUT.
A `float64` is created by taking this generated bit-pattern and zeroing out the lower 32 bits as follows.
Next, `1.0/sqrt(x)` is evaluated on the FPU to full precision.  The result is obtained as a `float64`
which is then "rounded up" by adding the constant `0x400` to it:

~~~
LUT[i] = to_int32(1.0/sqrt(x)) + 0x400
~~~

Why `0x400`?  To store the 8 most significant mantissa bits, they need to be rounded first.
Adding this (TODO) guarantees (TODO) they are correctly rounded within 10 bits:

~~~
  0eee eeee eeee mmmm mmmm rrrr rrrr rrrr
+ 0000 0000 0000 0000 0000 0100 0000 0000
  ---------------------------------------
  0eee eeee eeee mmmm mmmm
~~~

TODO

Now, since x87 uses 80-bit floating point internally, and the LUT only stores
the 8 most significant bits of the mantissa, this could easily be recompiled for SSE or something newer.  The
extra precision isn't used.

So now the LUT stores the rounded most significant 8 bits of the `float64` mantissa for `1.0/sqrt(x)`.

In the actual function, we can get the correct 8 bits... if we know which table entry to use.

First, lets assume our original `[0.5 .. 2)` input as it was.  Now we can immediately see that we can simply
index by ... TODO ... to obtain the 8 most significant mantissa bits of the result.

Theorem: ... TODO (take any FP ... )

TODO

OK, so we got our exponent bits.  We got our 8 mantissa bits.  We know the sign bit (0).  Now we can `or` these together to form a 
a complete `float64` number:

TODO

Nice!  But still not done - need to figure out the method at the bottom.  If exclude that, we still have an approximation,
but not a great one. (TODO)

I have a hunch it's one iteration of a Newton-Raphson approximation given the structure of the code, because we already
have a good starting value.

The math for Newton Raphson is ...

TODO

Indeed, the Carmack `Q_rsqrt` function is commonly characterized as being one iteration of Newton-Raphson with a good
first guess.

On paper, we get ...


This looks really close!  But what's the deal with this `1.00001` term?  It appears it is just a slight manual adjustment
to make it a bit better.  Indeed, multiplying by this makes `1.0` map to approximately `1.0`... TODO

The result is another variant of Newton-Raphson, astonishingly close to the Quake code except that the initial
first guess is obtained via a formula to get exponent bits and mantissa bits...

Now let's see how they compare.  We need to consider the range `[0.5 .. 2)`...

Let's iterate a lot of `float32`s in that range... this should be OK because the conversion from `float32` to `float64`
is non-lossy... TODO, rerun

Note that the method could be extended to increase the precision, but each extra bit doubles the size of the LUT!
And it doesn't really make sense to do this in new code. (TODO: warning about never using this on new processors).
But this function would have been a big deal in 1997, and possibly earlier if it was present in the original
Mechwarrior 2 (TODO).  Being able to quickly normalize vectors would make 3D graphics much easier to do
on the CPU, even if the results weren't perfect.

Now, let's put our theory into practice: let us replace this function in I'76 with an upgraded version that is both
faster and more accurate using `SSE2` (3? 4? TODO).

TODO: screenshot comparison

I've put the code on [GitHub](TODO) in case anyone wants to play around with the code.  Thanks for tuning in!
Next up: finding an old bug where ramming your car in Nitro would cause your health to reset (suspect it's an overflow bug).

# TESTMATH

Hello $$a^2=b^2$$ World

\begin{equation}
 match ado about nothing
\end{equation}

Hello {{< katex >}} a b c {{< /katex >}} world

Hello $a$ $b$ $cdef$ world

{{< katex display >}}
f(x) = \int_{-\infty}^\infty\hat f(\xi)\,e^{2 \pi i \xi x}\,d\xi [omg](omg) \tag{3}
{{< /katex >}}