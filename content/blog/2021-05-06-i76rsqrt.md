+++
title  = "Fast reciprocal square root... in 1997?!"
date = "2021-05-06"
tags = ["i76"]
draft = true
+++

This article is part of my series on reverse engineering [Interstate '76](https://en.wikipedia.org/wiki/Interstate_%2776), with my
current goal being to add a [Vulkan](https://en.wikipedia.org/wiki/Vulkan_(API)) renderer to the game.  I'm basing this work
directly on [UCyborg's patches](https://community.pcgamingwiki.com/files/file/1349-interstate-76-nitro-pack-aio-patch/) which include
many much-needed fixups to the game, including [my own patches]({{< relref "projects/interstate76anet.md" >}}) for the netcode.

# Introduction

Everyone is familiar with the famous [fast reciprocal square root function in the Quake
3 source code](https://en.wikipedia.org/wiki/Fast_inverse_square_root).  And, as noted
on Wikipedia, solutions have existed for computing the fast reciprocal square root
for many years before that, with perhaps the earliest implementation in 1986.

While I was working on reverse engineering Interstate '76, I discovered that Activision
had their own implementation in use from at least 1997, and that it shares some similarities
with the Quake 3 approach.  Interstate '76 is based off the Mechwarrior 2 engine --
If I had a copy of Mechwarrior 2 (DOS version) on hand, I could confirm
if this was present there as well, which would date this technique even
earlier!  In this post, I'll explain how it works.

# The TL;DR

The approach used in the Mechwarrior 2 engine computes an approximation of the reciprocal
square root using the x87 instruction set at `float64` (or `double`) precision.
At initialization
time, the engine generates a Lookup Table ([LUT](https://en.wikipedia.org/wiki/Lookup_table))
containing the 8 most significant mantissa bits of the reciprocal square root.  This LUT
has only 256 entries and covers the range TODO to TODO.  It is manually adjusted to
create more desirable characteristics for $[1 .. 2)$... (TODO).

The lookup table stores the most significant 8 mantissa bits of the reciprocal square root for the `float64` values with
hex representation of the upper 32-bits being `0x3FE00000` through ... (with the remaining lower 32 mantissa bits zeroed out).
This corresponds to the range `[0.5..2)`, `[0.5..1)` having a spacing of ... and `[1, 2)` having a spacing of ... TODO

What I find interesting is that the LUT wasn't precomputed and stored as a static array in the binary.  To be honest, 
I'm kind of glad -- it would have made this analysis a little harder to do!

TODO: INSERT PICTURE HERE OF THE LUT

Below is my own C++ version of the LUT generator code:

~~~

~~~

When compiled for x86 using only x87 instructions on MSVC 2019, it produces nearly identical assembly to the original.

The actual fast reciprocal square root function computes the result as follows:

1. Compute an initial guess for the result:
  * the exponent bits for the initial guess is determined through a formula that involves only addition and division by 2 (bit shift)
  * the mantissa bits for the initial guess is determined using the lookup table
  * `or` these values together to form a valid `float64` -- this is the guess
2. Perform one iteration of Newton-Raphson on the initial guess
3. Multiply the result by the constant $1.00001$

It is almost exactly the same as the Quake 3 approach except that the initial guess is computed differently.
It still uses Newton-Raphson with a manual adjustment afterwards.  I suspect the reason for the adjustment to be
that $1$ should ideally map to $1$ under it -- but without the multiplication by $1.00001$, it
maps to $0.99999...$ (TODO).  This retains more correct digits... the function seems to be
accurate to about 4 decimal places in the worst case from some brief testing I've done (TODO).

Below is my own C++ version of this function:

~~~

~~~

## Patched version of game with screenshots

TODO

So, I went ahead and replaced this with the actual platform native `rsqrt` in I'76.  Here are the results:
...

TODO PICTURES OF BEFORE AND AFTER

The patched code:
...

# The Long Version

#### Foreword

You know, back when I first started delving into reverse engineering I'76 ten years ago,
tools like [Ghidra](https://ghidra-sre.org/) weren't available.  The state of the art
was probably [IDA Pro](https://www.hex-rays.com/ida-pro/), a tool suite that while reasonably 
priced for what it did, was way out of the price range for me, a third year undergraduate
student.  Sure, pirating it was an option, but if you wanted to do things the honest way *and* on a budget, it was
hard to do better than [Ollydbg](http://www.ollydbg.de/version2.html), a free debugger
and disassembler for 32-bit x86.  Now, ten years later, the landscape looks radically
different.  There are so many fantastic tools available for doing this kind of thing,
and it has never been easier to mod your favourite games as a result.

## Introduction

I first discovered the code when identifying functions in the startup sequence of `nitro.exe`. Some fairly
normal initialization code, then I came across this function that populates a LUT using, seemingly, one byte from the result of a `float64` `1.0/sqrt(x)`.
Only one other function referenced it, and the code was almost incomprehensible at first glance:

TODO: PICTURE OF GHIDRA LUT GENERATION

TODO: PICTURE OF GHIDRA I76_RSQRT


Bit shifts, floats
being treated like integers, and other interesting things abound!  It *could* be what I was suspecting, but maybe
it was just a small piece of code used for something else.  Not wanting to jump to conclusions, I decided
to study it a bit more.  I figured a good place to start would be to see how this mysterious function was used.  I found
fragments of code like this all over the place:

TODO: PICTURE OF GHIDRA DOT PRODUCT WITH I76_RSQRT

"*Wow, that sure looks like a [dot product](https://en.wikipedia.org/wiki/Dot_product)*," I thought to myself.  If the function was being called
on the results of dot products, it's almost certain it's being used in vector normalization.  Indeed,
in the fragment of code above, you can see the vector components being multiplied by the result.
Could this really be an implementation of a fast reciprocal square root function similar to the Quake 3 approach
but in *1997*?  I had to try it!

Wanted to play with some new tools, I gave Cutter a try with it's emulation facilities to run the machine code directly, but unfortunately
couldn't figure out how to get it to show the `ST(i)` registers during execution.  So I moved on to
*x64dbg*, as it appeared to be a more modern version of *Ollydbg*.  After all, this isn't malware we're dealing with,
it's a game from 1997 -- what's the harm?  Probably that it would set my video mode to something weird and be a real pain
to fix.

I fired up x64dbg and set a breakpoint for right after the function where the LUT gets generated is called, then ran it.
Once execution reached the breakpoint, I immediately replaced the instruction pointer to the start of the function in question.
Seeing where the argument was passed in on the x87 stack, I figured why not try evaluating the value `2`?  So, I pushed it on the stack
and ran the function to completion:

...

Indeed, after running the function, `0.7071`... was sitting in `ST(0)`.  That's `1/sqrt(2)` accurate to about ... significant digits.
"*OK fine,*" I thought, "*let's try out some other values too, just to be sure.*" So, I tried out `100`, `4`, `64`, and `0.5`:

...

In each case, the result seemed to be accurate to within ... significant digits.

"*That's it.  I'm convinced!*", I said out loud.

"*But what the hell is it actually doing?!*"

The next step, I figured, would be to decompile it to C++ to help analyze it a bit easier.
This would be a manual process guided by Ghidra's own decompilation, which did a pretty good job, considering.
I wanted to compare it against a correctly rounded reciprocal square root to sample what the error
was across the board.

First, the C++ version of the LUT generator:

...

If you compile this for 32-bit with MSVC 2019, x87 only, you will get nearly identical assembly to what is in I'76:

...

This is acceptable.  Comparing the LUT generated here with the one visible in x64dbg yielded identical results,
so I was confident I got this right.  Now I could actually pick this apart a bit.  

{{< note >}}
The astute reader will notice that the LUT could have been computed entirely at compile time, or even just stored as
a static array in the executable.  I'm glad that Visual C++ 6.0 wasn't that smart, because it would have made this a fair bit harder to do!
{{< /note >}}

## Some basic analysis

I made a spreadsheet to see how this compares to the "precise" result...

Now let's see how they compare.  We need to consider the range `[0.5 .. 2)`...

Let's iterate a lot of `float32`s in that range... this should be OK because the conversion from `float32` to `float64`
is non-lossy... TODO, rerun

4 digits in the worst case?  Not bad!  Most of the time, you get 5 or more.
**Interesting that the error seems to repeat itself too...**

TODO: LINK TO RAW DATA

But how's this thing actually work?

## IEEE-754 `float64` refresher

A `float64` value is a 64-bit datatype of the following form, where the sign bit is the most significant bit:

~~~
s        eeeeeeeeeee mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
sign      exponent              mantissa
(1 bit)   (11 bits)             (52 bits)
~~~

TODO: use Wikipedia image

There are certain [special cases of this format](#ieee754-cases) that, for our purposes, we can completely ignore for our analysis.

For our purposes, the 11-bit exponent field is ... TODO ...
The 11-bit exponent field is stored *biased* by the value $1023$.

What this means is that $1023$ is added to the actual exponent before being
stored in the `float64`.  For example, the exponent $0$ is represented as just $1023$, and
the exponent $-1$ is represented using $1022$.

Another interesting aspect of this format is that the mantissa actually has an implicit $1$ added to it.
That is, if we treat the above mantissa field as an integer $z$, then the actual value of the mantissa is $1 + \frac{z}{2^52}$.

TODO...

Putting this together, a `float64` with biased exponent $\mathrm{bexp}$ and mantissa field integer $z$ represents the value:

{{< katex display >}}
2^{j - 1023}\left\( 1 + \frac{z}{2^52} \right\)
{{< /katex >}}

...

For example, the value $1$ is represented as follows:

~~~
0     01111111111 000.... 0
^     ^           ^
sign  exponent    mantissa
~~~

Or, in hexadecimal notation, `0x3FF0000000000000`. `float64` is kind of nice because the sign
and exponent bits end on a nibble boundary, so it's easy to see what the fractional
part of the mantissa is just by looking at the hexadecimal readout.

Note that x86 is little-endian, meaning the mantissa bits come first in memory, with the sign and exponent bits coming last:

TODO: colour code
~~~
seeeeeeeeeeemmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm

       7       6       5       4       3       2       1       0
(bytes in memory)
~~~

For example, byte 0 would contain the least significant 8 bits of the mantissa
while byte 7 would contain the sign bit and the most significant 7 bits of the biased exponent.

### x87 refresher

TODO

Now, back in 1997, things like SSE and AVX didn't quite exist yet.  I think MMX... might have
existed, but it wasn't something you could rely on being there.  What you most likely had
was the x87 instruction set to work with, and this thing came with a number of peculiarities.
Originally an optional co-processor on earlier Intel chipsets, it was integrated into the processor
proper in the ... this is reflected in the ISA itself.  It has it's own floating point stack,
and interacting with it involves pushing, reordering, and popping things from that stack.

It also curiously has it's own 80-bit floating point representation internally, which seems
to essentially be `float64` with extra mantissa bits and an explicit normalization bit.

## Assumptions about the input

As we know, an IEEE-754 `float64` is represented using a
sign, biased exponent, and mantissa.  Because we're dealing with approximations,
it's fair to make some assumptions about our input.  We assume the input is:

* non-`NaN`
* non-`InF` (i.e. finite)
* normal (i.e. not denormal/subnormal)
* positive (i.e., the sign bit is zero)
 
These assumptions simplify the math
and handle the vast majority of use cases that this function would be used for.  Remember:
this thing is supposed to be fast and reasonably accurate, not perfect.

## Notation

We can represent an IEEE-754 value that conforms to our assumptions as follows:

{{< katex display >}}
\newcommand{\fexp}{\mathit{exp}}
\newcommand{\ffrac}{\mathit{frac}}
\mathrm{fl}[v] = 2^\fexp(1 + \ffrac)
{{< /katex >}}

where $\mathrm{fl}[v] \in \mathbb{R}$ is the real floating point value, $\fexp \in \mathbb{Z}$ is the unbiased exponent,
and $\ffrac \in \mathbb{R}$ is the fractional part of the mantissa, $0 \leq \ffrac < 1$.

This is a [common notation](http://homepages.math.uic.edu/~hanson/mcs471/FloatingPointRep.html) used in many numerical analysis papers and
will help along our exploration.

Note that this notation places no bounds on precision or range of $fl[v]$.  To actually represent $\mathrm{fl}[v]$ in a `float64`, the following two conditions must hold:

1. $-1023 < \fexp < 1024$
2. $(\exists Z \in \mathbb{Z})\ \ffrac = \frac{z}{2^52}$

$\ffrac$ can be [rounded](https://en.wikipedia.org/wiki/Floating-point_arithmetic#Rounding_modes) to satisfy condition 2 -- in this case, the `float64` is an approximation
of the real value.

## LUT Generation analysis

First, the loop: the range `[0.5 .. 2)` is iterated 256 (TODO) times using this formula:

~~~
0 1111111111e mmmm mmm 0000 ...
~~~

What this means is half of the iterations are between `[0.5 .. 1)` and half are between `[1 .. 2)`:

* `[0.5 .. 1)` is iterated with an interval of TODO (when `e` is 0)
* `[1 .. 2)` is iterated with an interval of TODO (when `e` is 1)

`1.0/sqrt(x)` is evaluated with each input.  The lower 32 bits of the mantissa are completely thrown away,
The result is something that looks like a `float32`, but with more exponent bits.  This is nicer to work with
because the sign and exponent bits are 12 bits combined, meaning the mantissa is aligned on a nibble (4-bit) boundary.
This makes it very easy to understand when looking at the hexadecimal representation: 

~~~
0x3FE0 0000

  3FE
|    1 2345

sign bit: 0
exponent bits: 0x3FF (1023 biased) (0 unbiased)
mantissa bits: 0x12345
~~~

Each iteration of the loop populates one entry of the LUT.
A `float64` is created by taking this generated bit-pattern and zeroing out the lower 32 bits as follows.
Next, `1.0/sqrt(x)` is evaluated on the FPU to full precision.  The result is obtained as a `float64`
which is then "rounded up" by adding the constant `0x400` to it:

~~~
LUT[i] = to_int32(1.0/sqrt(x)) + 0x400
~~~

Why `0x400`?  To store the 8 most significant mantissa bits, they need to be rounded first.
Adding this (TODO) guarantees (TODO) they are correctly rounded upwards within 10 (TODO 8?) bits:

~~~
  0eee eeee eeee mmmm mmmm rrrr rrrr rrrr
+ 0000 0000 0000 0000 0000 0100 0000 0000
  ---------------------------------------
  0eee eeee eeee mmmm mmmm
~~~

TODO

Now, since x87 uses 80-bit floating point internally, and the LUT only stores
the 8 most significant bits of the mantissa, this could easily be recompiled for SSE or something newer.  The
extra precision isn't used.

So now the LUT stores the rounded most significant 8 bits of the `float64` mantissa for `1.0/sqrt(x)`.

In the actual function, we can get the correct 8 bits... if we know which table entry to use.

First, lets assume our original `[0.5 .. 2)` input as it was.  Now we can immediately see that we can simply
index by ... TODO ... to obtain the 8 most significant mantissa bits of the result.

So this is interesting.  The LUT contains TODO 256 entrites and the bottom 128 entries are used in the ODD (TODO) case.
the top X entries are used in the even case.


The odd entries have this form:

{{< katex display >}}
\mathrm{LUT}_{\mathrm{odd}}[i] = \frac{1}{\sqrt{\frac{1 + \frac{i}{128}}{2}}}
{{< /katex >}}

The even entries have this form:

{{< katex display >}}
\mathrm{LUT}_{\mathrm{even}}[i] = 2*\frac{1}{\sqrt{1 + \frac{i}{128}}}
{{< /katex >}}

Putting the two together:
{{< katex display >}}
\mathrm{LUT}[i] = \begin{cases} \mathrm{LUT}_{\mathrm{odd}}[i],\ \mathrm{if}\ i < 128 \\
  \mathrm{LUT}_{\mathrm{even}}[i - 128],\ \mathrm{otherwise} \end{cases}
{{< /katex >}}

Recall the mantissa shift... TODO...

In the actual function, we can get the correct 8 bits... if we know which table entry to use...

## i76_rsqrt

TODO: snippet of decompiled code and the math formula version...

OK, so we got our exponent bits.  We got our 8 mantissa bits.  We know the sign bit (0).  Now we can `or` these together to form a 
a complete `float64` number:

TODO

Nice!  But still not done - need to figure out the method at the bottom.  If exclude that, we still have an approximation,
but not a great one. (TODO)

I have a hunch it's one iteration of a Newton-Raphson approximation given the structure of the code, because we already
have a good starting value.

The math for Newton Raphson is ...

TODO

Indeed, the Quake 3 `Q_rsqrt` function is commonly characterized as being one iteration of Newton-Raphson with a good
first guess.

On paper, we get ... TODO

This looks really close!  But what's the deal with this `1.00001` term?  It appears it is just a slight manual adjustment
to make it a bit better.  Indeed, multiplying by this makes `1.0` map to approximately `1.0`... TODO

The result is another variant of Newton-Raphson, astonishingly close to the Quake code except that the initial
first guess is obtained via a formula to get exponent bits and mantissa bits...

## Forming the initial guess

The initial guess is also a `float64`.  Since it has three components (sign, biased exponent, and mantissa),
wouldn't it be nice if one could solve for each of them independently?
As it turns out, it is possible do just that!

### Sign bit

First, observe that the sign bit of the guess has to be 0, which denotes a non-negative number in IEEE-754.
This is reciprocal square root is always a positive value.  So that one was easy!  But we still
have the exponent and mantissa to worry about.

TODO: PICTURE HERE

### Exponent

One way to figure out what the exponent should be in our `float64` is to [take the floor of the base-2 logarithm of the value](#ieee754-decomp), denoted $\lg$:

{{< katex display >}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\floor{\lg\left(\frac{1}{\sqrt{v}}\right)} \\
=\floor{\lg(1) - \lg(\sqrt{v})} \\
=\floor{0 - \frac{\lg(v)}{2}} \\
=\floor{-\frac{\lg(2^\fexp(1 + \ffrac))}{2}} \\
=\floor{-\frac{\lg(2^\fexp)}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}}
{{< /katex >}}

Observe that $1 \leq 1 + \ffrac < 2$.  Therefore:

{{< katex display >}}
0 \leq \lg(1 + \ffrac) < 1 \\
\implies 0 \leq \frac{\lg(1 + \ffrac)}{2} < \frac{1}{2}
{{< /katex >}}

Also observe that $\fexp/2$ may not be an integer -- some odd/even analysis is in order.

#### Odd case

Assume that $\fexp$ is odd:

{{< katex display >}}
\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=\floor{\floor{-\frac{\fexp}{2}} + \frac{1}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=\floor{-\frac{\fexp}{2}} \\
=-\frac{\fexp}{2} - \frac{1}{2} \\
=-\frac{\fexp + 1}{2}
{{< /katex >}}

Note that $\fexp + 1$ is even, and therefore $-\frac{\fexp + 1}{2}$ is a valid exponent for IEEE-754.

#### Even cases

Now assume that $\fexp$ is even.  Therefore $\fexp/2$ is indeed an integer.
However, there are two cases to deal with: $\ffrac = 0$ and $\ffrac \neq 0$.  If $\ffrac = 0$ then:

{{< katex display >}}
\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=-\frac{\fexp}{2}
{{< /katex >}}

On the other hand, if $\ffrac \neq 0$ then :

{{< katex display >}}
\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=-\frac{\fexp + 2}{2}
{{< /katex >}}

### Putting it together

Let's recap.  We have three cases:
1. $\fexp$ is odd $\implies r_\fexp = -\frac{\fexp + 1}{2}$  
2. $\fexp$ is even and $\ffrac = 0 \implies r_\fexp = -\frac{\fexp}{2}$
3. $\fexp$ is even and $\ffrac \neq 0 \implies r_\fexp = -\frac{\fexp + 2}{2}$

Now we can directly determine what the exponent should be for the result.  What is especially cool
about this is that no square roots or divisions are needed to do this -- just an addition and a right shift!

### Biasing the exponent 

But we still need to bias the exponent!  
What would the biased exponent be?

1. $rexp_1 = -(\fexp + 1)/2$
2. $rexp_2 = -(\fexp/2)$
3. $rexp_3 = -(\fexp + 2)/2$

So the actual biased value would be:

1. $bexp_1 = rexp_1 + 1023 = -(\fexp + 1)/2 + 1023 = (2046 - \fexp - 1)/2 = (2045 - \fexp)/2$
2. $bexp_2 = rexp_2 + 1023 = -(\fexp/2) + 1023 = -(\fexp/2) + 2046/2 = (2046 - \fexp)/2$
3. $bexp_3 = rexp_3 + 1023 = -(\fexp/2 - 1) + 1023 = -(\fexp/2 - 1) + 2046/2 = (2044 - \fexp)/2$

And, notice we can take in a biased exponent as well, leading us to:

...

That looks a *lot* like our `0xBFC` constant, doesn't it?  

To keep it simple, they always selected the *odd* case, probably
probabilistically: it covers half of the possible inputs,
while the *even* cases split the remaining half depending on if $\ffrac = 0$.

Remember that branching
was likely expensive back in 1997 and before, as [out of order execution]()
and its cousin [speculative execution]() were not really a thing on x86 quite yet.
However, [superscalar]() execution definitely *was* a thing, since x87 coprocessor ran in parallel
with the the x86 core.  Since the pipeline was in-order, branching might have incurred a performance penalty
(a phenomenon TODO bubbling the pipeline).  And besides, this is just supposed to be an
initial guess anyway -- it doesn't need to be perfect.  If you had to hard-code a path to take, then selecting the most
probable case made sense.

Also note that TODO this means the actual result will be at most a factor of $\sqrt{2}$ off.
This is acceptable given that this is supposed to be just a first guess... TODO

TODO: picture here (unlocked!)

## Mantissa

But the exponent of our result alone won't do us much good.  We need the mantissa to complete our initial guess.
Let's take what we learned about the exponent and imagine if we took the reciprocal square root of $v$.
 What would it look like?

{{< katex display >}}
\frac{1}{\sqrt{v}} \\
= \frac{1}{\sqrt{2^\fexp(1 + \ffrac)}} \\
= \frac{1}{\sqrt{2^\fexp}\sqrt{1 + \ffrac}} \\
= 2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}}
{{< /katex >}}

Now this is interesting.
First, observe the following about: $\frac{1}{\sqrt{1 + \ffrac}}$:

{{< katex aligned >}}
&0 \leq \ffrac < 1 \\
\iff& 1 \leq 1 + \ffrac < 2 \\
\iff& 1 \leq \sqrt{1 + \ffrac} < \sqrt{2} \\
\iff& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} \leq 1 \tag{1}
{{< /katex >}}

TODO: SSO style, _odd, _even analysis, bring it together at the very end?

So by itself, this is not a suitable mantissa.  However, we know the exponent isn't quite right either, according
to the cases we've explored in the previous section.  Let's fix that and break this out on the cases outlined previously:

### $\fexp$ is odd

In this case, the floating point exponent of the guess is $-\frac{\fexp + 1}{2}$.  Let's work backwards to get there:

{{< katex aligned >}}
&2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1/2 + 1/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1/2}\frac{\sqrt{2}}{\sqrt{1 + \ffrac}} \\
=&2^{-(\fexp + 1)/2}\frac{1}{\frac{\sqrt{1 + \ffrac}}{\sqrt{2}}} \\
=&2^{-(\fexp + 1)/2}\frac{1}{\sqrt{\frac{1 + \ffrac}{2}}}
{{< /katex >}}

Note that, from $(1)$

{{< katex aligned >}}
& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} \leq 1 \\
\iff& 1 < \frac{\sqrt{2}}{\sqrt{1 + \ffrac}} \leq \sqrt{2}  \\
\implies& 1 \leq \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} < 2
{{< /katex >}}

This we can work with.  If we choose:

{{< katex display >}}
m = \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} - 1
{{< /katex >}}

Since $0 < m < 1$ and $\fexp + 1$ is even, then:

{{< katex display >}}
 2^{-(\fexp + 1)/2}(1+m)
{{< /katex >}}

is a valid IEEE-754 floating point value!  We have solved the problem for the odd case.
Next we will address the even cases.

TODO: verify mantissa fits... i doubt very much it does

### $\fexp$ is even and $\ffrac = 0$

Here, we know the floating point exponent of the guess is $-\frac{\fexp}{2}$.  Since $\ffrac = 0$, we already
know the mantissa of the result will be $1$:

{{< katex aligned >}}
&2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2}\frac{1}{\sqrt{1 + 0}} \\
=&2^{-\fexp/2}
{{< /katex >}}

That was easy!  Therefore, if we let $m = 0$, then

{{< katex display >}}
 2^{-(\fexp + 1)/2}(1+m)
{{< /katex >}}

is our `float64` result.

### $\fexp$ is even and $\ffrac \neq 0$

This is the hardest case to work with.  First, we know our exponent is $-\frac{\fexp + 2}{2}$

Let's work backwards once again:

{{< katex aligned >}}
&2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1 + 1}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1}\frac{2}{\sqrt{1 + \ffrac}} \\
{{< /katex >}}

Now here, $\ffrac \neq 0$, so $(1)$ simplifies to:

{{< katex display >}}
\frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} < 1
{{< /katex >}}

This is important as it allows the following:

{{< katex aligned >}}
& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} < 1 \\
\iff& \sqrt{2} < \frac{2}{\sqrt{1 + \ffrac}} < 2 \\
\implies& 1 \leq \frac{2}{\sqrt{1 + \ffrac}} < 2 
{{< /katex >}}

Therefore, if we set

{{< katex display >}}
m = \frac{2}{\sqrt{1 + \ffrac}} - 1
{{< /katex >}}

$0 \leq m < 1$ and:

{{< katex aligned  >}}
 =& 2^{-(\fexp + 2)/2}(1 + m)
{{< /katex >}}

is a valid IEEE-754 floating point value!. 

TODO: what happens if mantissa overflows? 0xFFF000 + 0x400... does this ever happen?

TODO NOMENCLATURE: significand includes 1?  adjust representation from (1 + m) to on paper?

TODO: are these looking familiar?  Recall section...

### Recap

Let's recap.  We have three cases for choosing the mantissa: TODO
1. $\fexp$ is odd $\implies r_\ffrac = ...$
2. $\fexp$ is even and $\ffrac = 0 \implies r_\ffrac = ...$
3. $\fexp$ is even and $\ffrac \neq 0 \implies r_\ffrac = ...$

This is all well and good for the pure math version, but still, where does $r_\ffrac$ actually come from?  

### Approximating the mantissa

Let's revisit what we know about the LUT:

...

Indeed, we see the *odd* case selects the lower half of the LUT while the *even* case selects the upper half...

Within this, we see the LUT stores approximations of ... in the lower half and of ... in the upper half.

Since we're constructing an initial guess, this loss of information is acceptable.   We could state:

...

Now we're talking.

TODO: PICTURE **ALL UNLOCKED**

We now have enough information to construct our first guess!

## LUT fixup

"Now that's all well and good", you say.  "But what happens when the value of 1.0 is passed in as input?"


Ah, yes, we have a bit of a problem here, don't we? `1.0` would have an exponent of 0 associated with it,
which is an even quantity.  Running it yields:

...

Well, that's not good.  Our LUT is correct, but we're choosing a lower exponent
than we should be due to forcing the *odd* case for exponent selection.
How can we fix that without branching?  Easy!  We just patch the LUT specifically
for that case:

...

Note the `0xFF` here -- this means we're setting the 8 most significant mantissa bits
to all ones.  Since the power is set to `-1` due to the *odd* rule working,
this means our true exponent is `0.5` -- so to try to get a value closer to 1, we 
set the mantissa bits to ones to compensate, bringing us closer:

...


TODO: picture here

Now our initial guess for `1.0` is much closer.  Indeed, this would affect anything...

TODO: affects specifically because of `0x00`?>

### Mantissa bits

Next, the LUT is used to get the mantissa bits of the result...

TODO: LUT CODE


First, the loop: the range `[0.5 .. 2)` is iterated 256 (TODO) times using this formula:

~~~
0 1111111111e mmmm mmm 0000 ...
~~~

What this means is half of the iterations are between `[0.5 .. 1)` and half are between `[1 .. 2)`:

* `[0.5 .. 1)` is iterated with an interval of TODO (when `e` is 0)
* `[1 .. 2)` is iterated with an interval of TODO (when `e` is 1)

`1.0/sqrt(x)` is evaluated with each input.  The lower 32 bits of the mantissa are completely thrown away,
The result is something that looks like a `float32`, but with more exponent bits.  This is nicer to work with
because the sign and exponent bits are 12 bits combined, meaning the mantissa is aligned on a nibble (4-bit) boundary.
This makes it very easy to understand when looking at the hexadecimal representation: 

~~~
0x3FE0 0000

  3FE
|    1 2345

sign bit: 0
exponent bits: 0x3FF (1023 biased) (0 unbiased)
mantissa bits: 0x12345
~~~

Each iteration of the loop populates one entry of the LUT.
A `float64` is created by taking this generated bit-pattern and zeroing out the lower 32 bits as follows.
Next, `1.0/sqrt(x)` is evaluated on the FPU to full precision.  The result is obtained as a `float64`
which is then "rounded up" by adding the constant `0x400` to it:

~~~
LUT[i] = to_int32(1.0/sqrt(x)) + 0x400
~~~

Why `0x400`?  To store the 8 most significant mantissa bits, they need to be rounded first.
Adding this (TODO) guarantees (TODO) they are correctly rounded within 10 bits:

~~~
  0eee eeee eeee mmmm mmmm rrrr rrrr rrrr
+ 0000 0000 0000 0000 0000 0100 0000 0000
  ---------------------------------------
  0eee eeee eeee mmmm mmmm
~~~

TODO

Now, since x87 uses 80-bit floating point internally, and the LUT only stores
the 8 most significant bits of the mantissa, this could easily be recompiled for SSE or something newer.  The
extra precision isn't used.

So now the LUT stores the rounded most significant 8 bits of the `float64` mantissa for `1.0/sqrt(x)`.

In the actual function, we can get the correct 8 bits... if we know which table entry to use.

First, lets assume our original `[0.5 .. 2)` input as it was.  Now we can immediately see that we can simply
index by ... TODO ... to obtain the 8 most significant mantissa bits of the result.

So this is interesting.  The LUT contains TODO 256 entrites and the bottom 128 entries are used in the ODD (TODO) case.
the top X entries are used in the even case.


The odd entries have this form:

{{< katex display >}}
\mathrm{LUT}_{\mathrm{odd}}[i] = \frac{1}{\sqrt{\frac{1 + \frac{i}{128}}{2}}}
{{< /katex >}}

The even entries have this form:

{{< katex display >}}
\mathrm{LUT}_{\mathrm{even}}[i] = 2*\frac{1}{\sqrt{1 + \frac{i}{128}}}
{{< /katex >}}

Putting the two together:
{{< katex display >}}
\mathrm{LUT}[i] = \begin{cases} \mathrm{LUT}_{\mathrm{odd}}[i],\ \mathrm{if}\ i < 128 \\
  \mathrm{LUT}_{\mathrm{even}}[i - 128],\ \mathrm{otherwise} \end{cases}
{{< /katex >}}

Recall the mantissa shift.

## Newton-Raphson iteration

Next, one iteration of Newton-Raphson is performed on the initial guess.  The derivation of this is as follows:

...

Looking at our pseudocode, we can see directly this being implemented with a strange extra multiplication of `1.00001`...

## Post-hoc fixup

Let's go back and try our `1.0` example again:

So we're pretty close, but notice we're still at `0.9999`.  This isn't a dealbreaker, but it *is* annoying.
How can we fix that?  Well, one way is to just apply a post-hoc fixup -- say, by scaling it up so it fits
better.  It only needs to be accurate to within 4 decimal places, so what do we do?  Multiply it by `1.00001`!

...

That's better.  Calling the function with `1.0` now gives you something that looks about right.  Note that this
could also have been achieved by adding this value as well.  I'm not sure what the tradeoff is -- it may have been
selected to improve the pipelining internally within the `x87` coprocessor.

## The big picture

Now we have a complete picture of how the Mechwarrior 2 engine evaluates the reciprocal square root.

LUT generation:
1. Cache $1/\sqrt{v}$ for v in $[0.5 .. 2)$ 
2. Apply post-hoc fixup for LUT[0x80]

Reciprocal square root function:
1. Get the mantissa bits from the LUT (adjusted) indexed by... (prove by theorem by index) TODO
2. Solve for the exponent bits using the formula (theorem) TODO
3. Combine these to form an initial guess
4. Perform 1 iteration of Newton-Raphson
5. Apply a post-hoc fixup multiplying the result by 1.0001 (to make 1.0 map to 1.0)

## Analysis

I tried out the decompiled version of the code by sampling inputs and comparing them with the
correctly rounded "ground truth" result.  The code appears to give about 5 significant digits of precision
in the worst case, often providing 6 or more.  Not bad at all!


The approach is almost exactly the same to the Quake 3 approach except that instead of
hard-coding an initial guess, the guess is obtained through a precomputed lookup table with 256 entries.
The lookup table stores the most significant 8 mantissa bits of the reciprocal square root for the `float64` values with
hex representation of the upper 32-bits being `0x3FE00000` through ... (with the remaining lower 32 mantissa bits zeroed out).
This corresponds to the range `[0.5..2)`, `[0.5..1)` having a spacing of ... and `[1, 2)` having a spacing of ...

TODO

## Patching the game

Now, let's put our theory into practice: let us replace this function in I'76 with an upgraded version that is both
faster and more accurate using `SSE2` (3? 4? TODO).

### Screenshots/video? TODO

## Conclusions

Note that the method could be extended to increase the precision, but each extra bit doubles the size of the LUT!
And it doesn't really make sense to do this in new code. (TODO: warning about never using this on new processors).
But this function would have been a big deal in 1997, and possibly earlier if it was present in the original
Mechwarrior 2 (TODO).  Being able to quickly normalize vectors would make 3D graphics much easier to do
on the CPU, even if the results weren't perfect.

I've put the code on [GitHub](TODO) in case anyone wants to play around with the code.  Thanks for tuning in!
Next up: finding an old bug where ramming your car in Nitro would cause your health to reset (suspect it's an overflow bug).

# In-browser demo

### LUT generation

TODO: IN BROWSER DEMO

## The `fast_rsqrt` function

#### Getting the correct exponent bits

TODO: interactive demo

#### Mantissa bits

TODO: interactive demo

#### Newton-Raphson

TODO: interactive demo

#### Fixup

TODO: interactive demo

Two fixups: 1.00001 and the LUT adjustment

# Appendix

## IEEE-754 special cases{#ieee754-cases}

Special cases of IEEE-754 binary datatypes:

* Zero is represented by zeroing out all bits in the `float64`.
* `Inf` is represented by setting all exponent bits to $1$ and all mantissa bits to $0$
  * The sign bit determines whether it is `+Inf` or `-Inf`
* [NaN](https://en.wikipedia.org/wiki/NaN) (*Not-a-Number*) is represented by setting all exponent bits to $1$
  * The sign bit determines whether the `NaN` is signaling or quiet
  * At least one mantissa bit is set to $0$
* [Subnormal numbers](https://en.wikipedia.org/wiki/Denormal_number) have all exponent bits set to $0$ with at least one mantissa bit set to $1$

## IEEE-754 float64 decomposition{#ieee754-decomp}

Let $x \in \mathbb{R}, x \geq 0$.  Then:

{{< katex display >}}
x = 2^\floor{\lg\left(x\right)}*\frac{x}{2^\floor{\lg\left(x\right)}}
{{< /katex >}}

This works because:

{{< katex aligned >}}
& 0 \leq \lg\left(x\right) - \floor{\lg\left(x\right)} < 1 \\
\implies& 2^0 \leq 2^{\lg\left(x\right) - \floor{\lg\left(x\right)}} < 2^1 \\
\implies& 1 \leq \frac{2^{\lg\left(x\right)}}{2^\floor{\lg\left(x\right)}} < 2 \\
\implies& 1 \leq \frac{x}{2^\floor{\lg\left(x\right)}} < 2
{{< /katex >}}

To represent this as a `float64`, two conditions need to be met:

* $-1023 < \floor{\lg\left(x\right)} < 1024$, otherwise this is an overflow or underflow condition (possibly requiring a subnormal)
* $\frac{x}{2^\floor{\lg\left(x\right)}} - 1$ can fit in the mantissa (52 bits), otherwise rounding is needed

Handling negative values is simple as well -- treat it as a positive value and then set the sign bit accordingly.

## Alternative formulation of exponent

Now, imagine if we took the reciprocal square root of $v$.  What would it look like?

{{< katex display >}}
\frac{1}{\sqrt{v}} 
= \frac{1}{\sqrt{2^\fexp(1 + \ffrac)}} 
= \frac{1}{\sqrt{2^\fexp}\sqrt{1 + \ffrac}} \\
= 2^{-\fexp/2}*\frac{1}{\sqrt{1 + \ffrac}}
{{< /katex >}}

Now this is interesting.  First, observe the following about: $\frac{1}{\sqrt{1 + \ffrac}}$:

{{< katex aligned >}}
&0 \leq \ffrac < 1 \\
\implies& 1 \leq 1 + \ffrac < 2 \\
\implies& 1 \leq \sqrt{1 + \ffrac} < \sqrt{2} \\
\implies& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} \leq 1 \\
\implies& 1 < \sqrt{2} < \frac{2}{\sqrt{1 + \ffrac}} \leq 2
{{< /katex >}}

Let's extract the fractional part of the mantissa above:

{{< katex display >}}
1 + m = \frac{2}{\sqrt{1 + \ffrac}}
{{< /katex >}}

Therefore $0 < m \leq 1$.
This is *very close* to being a valid mantissa for IEEE-754.  There's just the pesky
condition where $\ffrac = 0 \Rightarrow m = 1$.  Let's assume that $\ffrac \neq 0$:

{{< katex aligned  >}}
 &2^{-\fexp/2 - 1}\frac{2}{\sqrt{1 + \ffrac}} \\
 =& 2^{-\fexp/2 - 1}(1 + m) \\
 =& 2^{-(\fexp + 2)/2}(1 + m)
{{< /katex >}}

This is a valid floating point number!  Great!

But what if $\ffrac = 0$?  In this case, we must normalize the number by increasing the exponent by 1 and setting
the fractional part of the mantissa to 0:

{{< katex aligned >}}
 & 2^{-\fexp/2 - 1}(1 + m) \\
 =& 2^{-\fexp/2 - 1}(2) \\
 =& 2^{-\fexp/2}(1 + 0)
{{< /katex >}}

Next, observe that  $-1023 < -\fexp/2 < 1024$.  However, $-\fexp/2$ may not be an integer, unfortunately, but if it *was*,  then
this would be a valid exponent for a `float64`.

TODO ODD
But what if $\fexp$ isn't even?  Some additional analysis is in order.  Assume $\fexp$ is odd -- then $\fexp = 2p + 1$:

 {{< katex aligned >}}

 & 2^{-\fexp/2 - 1}(1 + m) \\
=& 2^{-(\fexp + 1 - 1)/2 - 1}(1 + m) \\
=& 2^{-(\fexp + 1)/2 + 1/2 - 1}(1 + m) \\
=& 2^{-(\fexp + 1)/2 - 1/2}(1 + m) \\
=& 2^{-(\fexp + 1)/2 - 1/2}\frac{2}{\sqrt{1 + \ffrac}} \\
=& 2^{-(\fexp + 1)/2}\frac{2}{\sqrt{2}\sqrt{1 + \ffrac}} \\
=& 2^{-(\fexp + 1)/2}\frac{\sqrt{2}}{\sqrt{1 + \ffrac}} \\
=& 2^{-(\fexp + 1)/2}\frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} \\

 {{< /katex >}}

Note that:

{{< katex display >}}
1 < \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} \leq \sqrt{2} < 2
{{< /katex >}}

In all cases -- regardless of if $\ffrac = 0$.

If we choose:

{{< katex display >}}
1 + m = \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}}
{{< /katex >}}

Since $0 < m < 1$ and $\fexp + 1$, then:

{{< katex display >}}
 2^{-(\fexp + 1)/2}(1+m)
{{< /katex >}}

is a valid IEEE-754 floating point value! 

TODO: what happens if mantissa overflows? 0xFFF000 + 0x400... does this ever happen?

TODO: discuss log approach?

TODO NOMENCLATURE: significand includes 1?  adjust representation from (1 + m) to on paper?

Let's recap.  We have three cases:
1. $\fexp$ is odd
2. $\fexp$ is even and $\ffrac = 0$
3. $\fexp$ is even and $\ffrac \neq 0$

Now we can directly determine what the exponent should be for the result.
What would the biased exponent be?

1. $rexp_1 = -(\fexp + 1)/2$
2. $rexp_2 = -(\fexp/2)$
3. $rexp_3 = -(\fexp + 2)/2$

So the actual biased value would be:

1. $bexp_1 = rexp_1 + 1023 = -(\fexp + 1)/2 + 1023 = (2046 - \fexp - 1)/2 = (2045 - \fexp)/2$
2. $bexp_2 = rexp_2 + 1023 = -(\fexp/2) + 1023 = -(\fexp/2) + 2046/2 = (2046 - \fexp)/2$
3. $bexp_3 = rexp_3 + 1023 = -(\fexp/2 - 1) + 1023 = -(\fexp/2 - 1) + 2046/2 = (2044 - \fexp)/2$
