+++
title = "How Activision implemented a fast reciprocal square root function in 1996"
date = "2021-05-06"
tags = ["i76"]
draft = true
+++

This article is part of my series on reverse engineering [Interstate '76](https://en.wikipedia.org/wiki/Interstate_%2776), with my
current goal being to add a [Vulkan](https://en.wikipedia.org/wiki/Vulkan_(API)) renderer to the game.

# Foreword

You know, back when I first started delving into reverse engineering I'76 ten years ago,
tools like [Ghidra](https://ghidra-sre.org/) weren't available.  The state of the art
was probably [IDA Pro](https://www.hex-rays.com/ida-pro/), a tool suite that while reasonably 
priced for what it did, was way out of the price range for me, a third year undergraduate
student.  Sure, pirating it was an option, but if you wanted to do things the honest way *and* on a budget, it was
hard to do better than [Ollydbg](http://www.ollydbg.de/version2.html), a free debugger
and disassembler for 32-bit x86.  Now, ten years later, the landscape looks radically
different.  There are so many fantastic tools available for doing this kind of thing,
and it has never been easier to mod your favourite games as a result.

# Introduction

Everyone is familiar with the famous [fast reciprocal square root function in the Quake
3 source code](https://en.wikipedia.org/wiki/Fast_inverse_square_root).  And, as noted
on Wikipedia, solutions have existed for computing the fast reciprocal square root
for many years before that, with perhaps the earliest implementation in 1986.

While I was working on reverse engineering Interstate '76, I discovered that Activision
had their own implementation in use from at least 1997, and it shares some similarities
with the Quake 3 approach.  If I had a copy of Mechwarrior 2 (DOS version) on hand, I could confirm
if this was present there as well, which would date this technique even
earlier.  In this post, I'll explain how it works.

# The TL;DR

The approach used in the Mechwarrior 2 engine computes an approximation of the reciprocal
square root using the x87 instruction set at `float64` (or `double`) precision.  At initialization
time, the engine generates a Lookup Table ([LUT](https://en.wikipedia.org/wiki/Lookup_table))
containing the 8 most significant mantissa bits of the reciprocal square root.  This LUT
has only 256 entries and covers the range TODO to TODO.  It is manually adjusted to
create more desirable characteristics around the value `1.0` (TODO).

TODO: INSERT PICTURE HERE

Later, the actual reciprocal square root function uses this LUT to help form
an initial guess for the result.

## Representation

TODO: what does a `float64` look like?  put diagrams here!

IEEE-754 stores the exponent of the number using a *biasing* value.  For `float64`, the biasing
value is $1023$.  What this means is that $1023$ is added to the actual exponent before being
stored in the `float64`.  For example, the exponent $0$ is represented as just $1023$, and
the exponent $-1$ is represented using $1022$.

## Assumptions about the input

As we know, an IEEE-754 `float64` is represented using a
sign, biased exponent, and mantissa.  Because we're dealing with approximations,
it's fair to make some assumptions about our input.  We assume the input is:

* non-`NaN`
* non-`InF` (i.e. finite)
* normal (i.e. not denormal/subnormal)
* positive (i.e., the sign bit is zero)
 
These assumptions simplify the math
and handle the vast majority of use cases that this function would be used for.  Remember:
this thing is supposed to be fast and reasonably accurate, not perfect.

## Notation

We can represent a `float64` $v$ that conforms to our assumptions as follows:

{{< katex display >}}
\newcommand{\fexp}{\mathit{exp}}
\newcommand{\ffrac}{\mathit{frac}}
\mathrm{fl}[v] = 2^\fexp(1 + \ffrac)
{{< /katex >}}

where $\mathrm{fl}[v] \in \mathbb{R}$ is the real floating point value, $\fexp \in \mathbb{Z}$, $-1023 < \fexp < 1024$ is the unbiased exponent,
and $\ffrac \in \mathbb{Q}$ with denominator $2^{52}$ is the fractional part of the mantissa, $0 \leq \ffrac < 1$.  

This is a [common notation](http://homepages.math.uic.edu/~hanson/mcs471/FloatingPointRep.html) used in many numerical analysis papers and
will help along our exploration.

TODO: euclidean division 2p + 1 remainder....

## Forming the initial guess

The initial guess is also a `float64`.  Since it has three components (sign, biased exponent, and mantissa),
wouldn't it be nice if one could solve for each of them independently?
As it turns out, it is possible do just that!

### Sign bit

First, observe that the sign bit of the guess has to be 0, which denotes a non-negative number in IEEE-754.
This is reciprocal square root is always a positive value.  So that one was easy!  But we still
have the exponent and mantissa to worry about.

TODO: PICTURE HERE

### Exponent

One way to figure out what the exponent should be in our `float64` is to [take the base-2 logarithm of the value](#ieee754-decomp), denoted $\lg$:

{{< katex display >}}
\newcommand{\floor}[1]{{\left\lfloor #1 \right\rfloor}}

\floor{\lg\left(\frac{1}{\sqrt{v}}\right)} \\
=\floor{\lg(1) - \lg(\sqrt{v})} \\
=\floor{0 - \frac{\lg(v)}{2}} \\
=\floor{-\frac{\lg(2^\fexp(1 + \ffrac))}{2}} \\
=\floor{-\frac{\lg(2^\fexp)}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}}
{{< /katex >}}

Observe that $1 \leq 1 + \ffrac < 2$.  Therefore:

{{< katex display >}}
0 \leq \lg(1 + \ffrac) < 1 \\
\implies 0 \leq \frac{\lg(1 + \ffrac)}{2} < \frac{1}{2}
{{< /katex >}}

Also observe that $\fexp/2$ may not be an integer -- some odd/even analysis is in order.

#### Odd case

Assume that $\fexp$ is odd:

{{< katex display >}}
\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=\floor{\floor{-\frac{\fexp}{2}} + \frac{1}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=\floor{-\frac{\fexp}{2}} \\
=-\frac{\fexp}{2} - \frac{1}{2} \\
=-\frac{\fexp + 1}{2}
{{< /katex >}}

Note that $\fexp + 1$ is even, and therefore $-\frac{\fexp + 1}{2}$ is a valid exponent for IEEE-754.

#### Even cases

Now assume that $\fexp$ is even.  Therefore $\fexp/2$ is indeed an integer.
However, there are two cases to deal with: $\ffrac = 0$ and $\ffrac \neq 0$.  If $\ffrac = 0$ then:

{{< katex display >}}
\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=-\frac{\fexp}{2}
{{< /katex >}}

On the other hand, if $\ffrac \neq 0$ then :

{{< katex display >}}
\floor{- \frac{\fexp}{2} - \frac{\lg(1 + \ffrac))}{2}} \\
=-\frac{\fexp + 2}{2}
{{< /katex >}}

### Putting it together

Let's recap.  We have three cases:
1. $\fexp$ is odd $\implies r_\fexp = -\frac{\fexp + 1}{2}$  
2. $\fexp$ is even and $\ffrac = 0 \implies r_\fexp = -\frac{\fexp}{2}$
3. $\fexp$ is even and $\ffrac \neq 0 \implies r_\fexp = -\frac{\fexp + 2}{2}$

Now we can directly determine what the exponent should be for the result.  What is especially cool
about this is that no square roots or divisions are needed to do this -- just an addition and a right shift!

TODO: picture here (unlocked!)

## Mantissa

But the exponent of our result alone won't do us much good.  We need the mantissa to complete our initial guess.
Let's take what we learned about the exponent and imagine if we took the reciprocal square root of $v$.
 What would it look like?

{{< katex display >}}
\frac{1}{\sqrt{v}} \\
= \frac{1}{\sqrt{2^\fexp(1 + \ffrac)}} \\
= \frac{1}{\sqrt{2^\fexp}\sqrt{1 + \ffrac}} \\
= 2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}}
{{< /katex >}}

Now this is interesting.
First, observe the following about: $\frac{1}{\sqrt{1 + \ffrac}}$:

{{< katex aligned >}}
&0 \leq \ffrac < 1 \\
\iff& 1 \leq 1 + \ffrac < 2 \\
\iff& 1 \leq \sqrt{1 + \ffrac} < \sqrt{2} \\
\iff& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} \leq 1 \tag{1}
{{< /katex >}}

TODO: SSO style, _odd, _even analysis, bring it together at the very end?

So by itself, this is not a suitable mantissa.  However, we know the exponent isn't quite right either, according
to the cases we've explored in the previous section.  Let's fix that and break this out on the cases outlined previously:

### $\fexp$ is odd

In this case, the floating point exponent of the guess is $-\frac{\fexp + 1}{2}$.  Let's work backwards to get there:

{{< katex aligned >}}
&2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1/2 + 1/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1/2}\frac{\sqrt{2}}{\sqrt{1 + \ffrac}} \\
=&2^{-(\fexp + 1)/2}\frac{1}{\frac{\sqrt{1 + \ffrac}}{\sqrt{2}}} \\
=&2^{-(\fexp + 1)/2}\frac{1}{\sqrt{\frac{1 + \ffrac}{2}}}
{{< /katex >}}

Note that, from $(1)$

{{< katex aligned >}}
& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} \leq 1 \\
\iff& 1 < \frac{\sqrt{2}}{\sqrt{1 + \ffrac}} \leq \sqrt{2}  \\
\implies& 1 \leq \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} < 2
{{< /katex >}}

This we can work with.  If we choose:

{{< katex display >}}
m = \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} - 1
{{< /katex >}}

Since $0 < m < 1$ and $\fexp + 1$ is even, then:

{{< katex display >}}
 2^{-(\fexp + 1)/2}(1+m)
{{< /katex >}}

is a valid `float64`!  We have solved the problem for the odd case.
Next we will address the even cases.

TODO: verify mantissa fits... i doubt very much it does

### $\fexp$ is even and $\ffrac = 0$

Here, we know the floating point exponent of the guess is $-\frac{\fexp}{2}$.  Since $\ffrac = 0$, we already
know the mantissa of the result will be $1$:

{{< katex aligned >}}
&2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2}\frac{1}{\sqrt{1 + 0}} \\
=&2^{-\fexp/2}
{{< /katex >}}

That was easy!  Therefore, if we let $m = 0$, then

{{< katex display >}}
 2^{-(\fexp + 1)/2}(1+m)
{{< /katex >}}

is our `float64` result.

### $\fexp$ is even and $\ffrac \neq 0$

This is the hardest case to work with.  First, we know our exponent is $-\frac{\fexp + 2}{2}$

Let's work backwards once again:

{{< katex aligned >}}
&2^{-\fexp/2}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1 + 1}\frac{1}{\sqrt{1 + \ffrac}} \\
=&2^{-\fexp/2 - 1}\frac{2}{\sqrt{1 + \ffrac}} \\
{{< /katex >}}

Now here, $\ffrac \neq 0$, so $(1)$ simplifies to:

{{< katex display >}}
\frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} < 1
{{< /katex >}}

This is important as it allows the following:

{{< katex aligned >}}
& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} < 1 \\
\iff& \sqrt{2} < \frac{2}{\sqrt{1 + \ffrac}} < 2 \\
\implies& 1 \leq \frac{2}{\sqrt{1 + \ffrac}} < 2 
{{< /katex >}}

Therefore, if we set

{{< katex display >}}
m = \frac{2}{\sqrt{1 + \ffrac}} - 1
{{< /katex >}}

$0 \leq m < 1$ and:

{{< katex aligned  >}}
 =& 2^{-(\fexp + 2)/2}(1 + m)
{{< /katex >}}

is a valid `float64`. TODO: verify mantissa fits... i doubt very much it does


TODO: what happens if mantissa overflows? 0xFFF000 + 0x400... does this ever happen?

TODO: discuss log approach?

TODO NOMENCLATURE: significand includes 1?  adjust representation from (1 + m) to on paper?

### Recap

Let's recap.  We have three cases for choosing the mantissa: TODO
1. $\fexp$ is odd $\implies r_\ffrac = ...$
2. $\fexp$ is even and $\ffrac = 0 \implies r_\ffrac = ...$
3. $\fexp$ is even and $\ffrac \neq 0 \implies r_\ffrac = ...$

TODO: PICTURE **ALL UNLOCKED**

### Biasing the exponent 

TODO: move this

But we still need to bias the exponent!  
What would the biased exponent be?

1. $rexp_1 = -(\fexp + 1)/2$
2. $rexp_2 = -(\fexp/2)$
3. $rexp_3 = -(\fexp + 2)/2$

So the actual biased value would be:

1. $bexp_1 = rexp_1 + 1023 = -(\fexp + 1)/2 + 1023 = (2046 - \fexp - 1)/2 = (2045 - \fexp)/2$
2. $bexp_2 = rexp_2 + 1023 = -(\fexp/2) + 1023 = -(\fexp/2) + 2046/2 = (2046 - \fexp)/2$
3. $bexp_3 = rexp_3 + 1023 = -(\fexp/2 - 1) + 1023 = -(\fexp/2 - 1) + 2046/2 = (2044 - \fexp)/2$


To put this in the context of the actual implementation in I'76:

To keep it simple, Activision always selected the first case, probably
probabilistically: the *odd* case covers half of the possible inputs,
while the *even* cases split the remaining halves.  Remember that branching
was likely expensive back in 1997 and before, as [out of order execution]()
and its cousin [speculative execution]() were not really a thing on x86 quite yet.
However, [superscalar]() execution definitely *was* a thing, since x87 coprocessor ran in parallel
with the the x86 core.  Since the pipeline was in-order, branching might have incurred a performance penalty
(a phenomenon TODO bubbling the pipeline).  And besides, this is just supposed to be an
initial guess anyway -- it doesn't need to be perfect.  If you had to hard-code a path to take, then selecting the most
probable case made sense.

"Now that's all well and good", you say.  "But what happens when the value of 1.0 is passed in as input?"

TODO: power vs exponent

Ah, yes, we have a bit of a problem here, don't we? `1.0` would have an exponent of 0 associated with it,
which is an even quantity.  Running it yields:

...

TODO: discuss mantissa recovery before this section

Well, that's not good.  Our LUT is correct, but we're choosing a lower exponent
than we should be due to forcing the *odd* case for exponent selection.
How can we fix that without branching?  Easy!  We just patch the LUT specifically
for that case:

...

Note the `0xFF` here -- this means we're setting the 8 most significant mantissa bits
to all ones.  Since the power is set to `-1` due to the *odd* rule working,
this means our true exponent is `0.5` -- so to try to get a value closer to 1, we 
set the mantissa bits to ones to compensate, bringing us closer:

...


TODO: picture here

### Mantissa bits

Next, the LUT is used to get the mantissa bits of the result...

TODO: LUT CODE


First, the loop: the range `[0.5 .. 2)` is iterated 256 (TODO) times using this formula:

~~~
0 1111111111e mmmm mmm 0000 ...
~~~

What this means is half of the iterations are between `[0.5 .. 1)` and half are between `[1 .. 2)`:

* `[0.5 .. 1)` is iterated with an interval of TODO (when `e` is 0)
* `[1 .. 2)` is iterated with an interval of TODO (when `e` is 1)

`1.0/sqrt(x)` is evaluated with each input.  The lower 32 bits of the mantissa are completely thrown away,
The result is something that looks like a `float32`, but with more exponent bits.  This is nicer to work with
because the sign and exponent bits are 12 bits combined, meaning the mantissa is aligned on a nibble (4-bit) boundary.
This makes it very easy to understand when looking at the hexadecimal representation: 

~~~
0x3FE0 0000

  3FE
|    1 2345

sign bit: 0
exponent bits: 0x3FF (1023 biased) (0 unbiased)
mantissa bits: 0x12345
~~~

Each iteration of the loop populates one entry of the LUT.
A `float64` is created by taking this generated bit-pattern and zeroing out the lower 32 bits as follows.
Next, `1.0/sqrt(x)` is evaluated on the FPU to full precision.  The result is obtained as a `float64`
which is then "rounded up" by adding the constant `0x400` to it:

~~~
LUT[i] = to_int32(1.0/sqrt(x)) + 0x400
~~~

Why `0x400`?  To store the 8 most significant mantissa bits, they need to be rounded first.
Adding this (TODO) guarantees (TODO) they are correctly rounded within 10 bits:

~~~
  0eee eeee eeee mmmm mmmm rrrr rrrr rrrr
+ 0000 0000 0000 0000 0000 0100 0000 0000
  ---------------------------------------
  0eee eeee eeee mmmm mmmm
~~~

TODO

Now, since x87 uses 80-bit floating point internally, and the LUT only stores
the 8 most significant bits of the mantissa, this could easily be recompiled for SSE or something newer.  The
extra precision isn't used.

So now the LUT stores the rounded most significant 8 bits of the `float64` mantissa for `1.0/sqrt(x)`.

In the actual function, we can get the correct 8 bits... if we know which table entry to use.

First, lets assume our original `[0.5 .. 2)` input as it was.  Now we can immediately see that we can simply
index by ... TODO ... to obtain the 8 most significant mantissa bits of the result.

So this is interesting.  The LUT contains TODO 256 entrites and the bottom 128 entries are used in the ODD (TODO) case.
the top X entries are used in the even case.


The odd entries have this form:

{{< katex display >}}
\mathrm{LUT}_{\mathrm{odd}}[i] = \frac{1}{\sqrt{\frac{1 + \frac{i}{128}}{2}}}
{{< /katex >}}

The even entries have this form:

{{< katex display >}}
\mathrm{LUT}_{\mathrm{even}}[i] = 2*\frac{1}{\sqrt{1 + \frac{i}{128}}}
{{< /katex >}}

Putting the two together:
{{< katex display >}}
\mathrm{LUT}[i] = \begin{cases} \mathrm{LUT}_{\mathrm{odd}}[i],\ \mathrm{if}\ i < 128 \\
  \mathrm{LUT}_{\mathrm{even}}[i - 128],\ \mathrm{otherwise} \end{cases}
{{< /katex >}}

Recall the mantissa shift.

### The big picture

Now we have a complete picture of how to form a guess:

1. Get the mantissa bits from the LUT (adjusted) indexed by... (prove by theorem by index)
2. Solve for the exponent bits using the formula (theorem)
3. Combine these to form an initial guess

## Newton-Raphson iteration

Next, one iteration of Newton-Raphson is performed on the initial guess.  The derivation of this is as follows:

...

Looking at our pseudocode, we can see directly this being implemented with a strange extra multiplication of `1.00001`...

## Post-hoc fixup

Let's go back and try our `1.0` example again:

So we're pretty close, but notice we're still at `0.9999`.  This isn't a dealbreaker, but it *is* annoying.
How can we fix that?  Well, one way is to just apply a post-hoc fixup -- say, by scaling it up so it fits
better.  It only needs to be accurate to within 4 decimal places, so what do we do?  Multiply it by `1.00001`!

...

That's better.  Calling the function with `1.0` now gives you something that looks about right.


## Analysis

I tried out the decompiled version of the code by sampling inputs and comparing them with the
correctly rounded "ground truth" result.  The code appears to give about 5 significant digits of precision
in the worst case, often providing 6 or more.  Not bad at all!

### Similarities with the Quake 3 code

The approach is almost exactly the same to the Quake 3 approach except that instead of
hard-coding an initial guess, the guess is obtained through a precomputed lookup table with 256 entries.
The lookup table stores the most significant 8 mantissa bits of the reciprocal square root for the `float64` values with
hex representation of the upper 32-bits being `0x3FE00000` through ... (with the remaining lower 32 mantissa bits zeroed out).
This corresponds to the range `[0.5..2)`, `[0.5..1)` having a spacing of ... and `[1, 2)` having a spacing of ...

The initial guess is already decent, but it is further
refined using one iteration of the [Newton-Raphson method](https://en.wikipedia.org/wiki/Newton%27s_method)
to get a closer result, accurate to about 4 decimal places in the absolute worst case.
Two post-hoc "fixups" are present: one in the generation of the lookup table, and one
after the iteration of Newton-Raphson is completed.

The approach is almost exactly the same to the Quake 3 approach except that instead of
hard-coding an initial guess, the guess is obtained through a precomputed lookup table with 256 entries.
The lookup table stores the most significant 8 mantissa bits of the reciprocal square root for the `float64` values with
hex representation of the upper 32-bits being `0x3FE00000` through ... (with the remaining lower 32 mantissa bits zeroed out).
This corresponds to the range `[0.5..2)`, `[0.5..1)` having a spacing of ... and `[1, 2)` having a spacing of ...


# The long version

As part of my work on trying to add a Vulkan backend to I'76, I stumbled upon an interesting little
function in the initialization sequence of the game.  It appeared to be generating some kind of LUT
using the result of a a reciprocal square root:

...

Hmm.  Interesting.  Looks like it's only caching the mantissa bits.  But why only 8 of them?

After looking where else this was used, I found only one reference in this function:

...

Wow, what on earth?  Is this thing supposed to compute a reciprocal square root?  Let's see how this function is used:

...

That basically does it -- this thing is being called on things that look like [dot products](https://en.wikipedia.org/wiki/Dot_product), which means it's likely
being used as part of vector normalization.  Let's try to run this thing ourselves, shall we?

...

Unfortunately, Cutter's emulation capabilities seem to not show the x87 stack (or at least, I'm not aware of how to get that to show).
But this isn't malware we're dealing with, it's a game from 1996.  So, let's try it in x86dbg instead:

...

That's better.  Let's try a few values, shall we?

First, we'll set a breakpoint to let the game initialize and then run all the way until the LUT is finished generating.
Then, we'll manully modify EIP (or RIP), the instruction pointer, to point to the start of the function.  Now,
we can populate the stack with whatever we want.  Let's try `1.0`:

...

Cool!  Let's try 0.5 -- this should give us the square root of 2:

...

Not bad!  OK, let's try some other powers of 2:

...

Neat!  It actually seems to do a pretty decent job.  
Ghidra's decompiler was very helpful for providing a quick overview of what's going on,
but for the sake of my understanding, I rewrote it in C++:

~~~

~~~

Now we can actually test it out in bulk.  This compiles to almost exactly
the same instructions present in I'76 when built in 32-bit, only x87 allowed.

### Some basic analysis

I made a spreadsheet to see how this compares to the "precise" result...

4 digits in the worst case?  Not bad!  Most of the time, you get 5 or more.
**Interesting that the error seems to repeat itself too...**

TODO: LINK TO RAW DATA



But how's this thing actually work?

## The lookup table

Let's first figure out exactly what's going on with the LUT.  For a refresher, this is how
a `float64` is represented:

### float64 refresher

~~~
s        eeeeeeeeeee mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm
sign      exponent              mantissa
(1 bit)   (11 bits)             (52 bits)
~~~

The sign bit is `0` if the float is positive, and `1` if it is negative.
The exponent bits represent a *biased exponent*, with the number 1023 added into it.
This means that `1.0` is represented as follows:

~~~
0 11111111111 000.... 0
~~~

Or, in hex notation, `0x3FF0000000000000`. `float64` is kind of nice because the sign
and exponent bits end on a nibble boundary, so ...

Unlike `float32`...

IEEE floating point supports some special values like `+Inf`, `-Inf`, and `NaN`.
`NaN` is actually an entire range of values...

There is also the special case of *subnormal* (or *denormal*) values to deal with...

x86 is little-endian, meaning the mantissa bits come first in memory, with the sign and exponent bits coming last:

TODO: colour code
~~~
seeeeeeeeeeemmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm

       7       6       5       4       3       2       1       0
(bytes in memory)
~~~

So, byte 0 would correspond to the least significant 8 bits of the mantissa.

To keep things simple, we will assume our input is non-`NaN`, non-`Inf`, and normalized.
We're dealing with an approximation of a function, after all -- it doesn't have to be perfect,
 it just has to be good enough for the cases it has to work with.

### x87 refresher

Now, back in 1996, things like SSE and AVX didn't quite exist yet.  I think MMX... might have
existed, but it wasn't something you could rely on being there.  What you most likely had
was the x87 instruction set to work with, and this thing came with a number of peculiarities.
Originally an optional co-processor on earlier Intel chipsets, it was integrated into the processor
proper in the ... this is reflected in the ISA itself.  It has it's own floating point stack,
and interacting with it involves pushing, reordering, and popping things from that stack.

It also curiously has it's own 80-bit floating point representation internally, which seems
to essentially be `float64` with extra mantissa bits and an explicit normalization bit.

### LUT generation

The LUT is generated by this...

TODO: IN BROWSER DEMO

... Note the "fixup" at the end.  We'll see what this is for later.

## The `fast_rsqrt` function

### The "what"

Combining exponent bits with mantissa bits to get the guess

#### Getting the correct exponent bits

TODO: interactive demo

#### Recovering the correct mantissa bits from the LUT

### The "how"

Let's run through a few cases.  First, let's assume that our input is in `[0.5, 2)`...
As we can see, the mantissa bits are correctly recovered from the LUT and the exponent is correctly
calculated.

### The "fixups"

Two fixups: 1.00001 and the LUT adjustment


# Other stuff

The astute reader will notice that the LUT could have been computed entirely at compile time, or even just stored as
an array initially.  I'm glad that Visual C++ 6.0 wasn't that smart, because it would have made this a fair bit harder to do!


# Junk from my notes to fill in where needed

This approach pre-dates the Quake 3 method by about 3 years...

My work builds on UCyborg's work.

I first discovered the code when identifying functions in the startup sequence of `nitro.exe`. Some fairly
normal initialization code, then suddently this function that populates a LUT using, seemingly, one byte from the result of a `float64` `1.0/sqrt(x)` appears.
Only one other function referenced it, and the code was almost incomprehensible at first glance.  Bit shifts, floats
being treated like integers, and other interesting things abound.  It *could* be what I was suspecting, but maybe
it was just a small piece of code used for something else.  Not wanting to jump to conclusions, I decided
to study it a bit more.  I figured a good place to start would be to see how this mysterious function was used.  I found
fragments of code like this all over the place:

...

"*Wow, that sure looks like a dot product*," I thought to myself.  If the function was being called
on the results of dot products, it's almost certain it's being used in vector normalization.  Indeed,
in the fragment of code above, you can see the vector components being multiplied by the result.
Could this really be an implementation of a fast reciprocal square root function similar to the Quake 3 approach
but in *1997*?  I had to try it!

Wanted to play with some new tools, I gave *x64dbg* a spin as it appears to be a more modern
version of Ollydbg.  I set a breakpoint for right after the function where the LUT gets generated,
and then immediately replaced the instruction pointer to the start of the function.
Seeing where the argument was passed in on the x87 stack, I figured why not try the value `2`:

...

Indeed, after running the function, `0.7071`... was sitting in `ST(0)`.  That's `1/sqrt(2)` accurate to about ... significant digits.
"*OK fine,*" I thought, "*let's try out some other values too, just to be sure.*" So, I tried out `100`, `4`, `64`, and `0.5`:

...

In each case, the result seemed to be accurate to within ... significant digits.

"*That's it.  I'm convinced!*", I said out loud, "*But what the hell is it actually doing?!*"

The next step, I figured, would be to decompile it to C++ to help analyze it a bit easier.
This would be a manual process guided by Ghidra's own decompilation, which *mostly* got it right.
I wanted to compare it against a correctly rounded reciprocal square root to sample what the error
was across the board.

First, the decompiled LUT generator:

...

If you compile this for 32-bit with MSVC 2019, x87 only, you will get nearly identical assembly to what is in I'76:

...

This is acceptable.  Comparing the LUT generated here with the one visible in x64dbg yielded identical results,
so I was confident I got this right.  Now we can actually pick this apart a bit.  

TODO: `float64` blah blah

TODO: copied

First, the loop: the range `[0.5 .. 2)` is iterated 256 (TODO) times using this formula:

~~~
0 1111111111e mmmm mmm 0000 ...
~~~

What this means is half of the iterations are between `[0.5 .. 1)` and half are between `[1 .. 2)`:

* `[0.5 .. 1)` is iterated with an interval of TODO (when `e` is 0)
* `[1 .. 2)` is iterated with an interval of TODO (when `e` is 1)

`1.0/sqrt(x)` is evaluated with each input.  The lower 32 bits of the mantissa are completely thrown away,
The result is something that looks like a `float32`, but with more exponent bits.  This is nicer to work with
because the sign and exponent bits are 12 bits combined, meaning the mantissa is aligned on a nibble (4-bit) boundary.
This makes it very easy to understand when looking at the hexadecimal representation: 

~~~
0x3FE0 0000

  3FE
|    1 2345

sign bit: 0
exponent bits: 0x3FF (1023 biased) (0 unbiased)
mantissa bits: 0x12345
~~~

Each iteration of the loop populates one entry of the LUT.
A `float64` is created by taking this generated bit-pattern and zeroing out the lower 32 bits as follows.
Next, `1.0/sqrt(x)` is evaluated on the FPU to full precision.  The result is obtained as a `float64`
which is then "rounded up" by adding the constant `0x400` to it:

~~~
LUT[i] = to_int32(1.0/sqrt(x)) + 0x400
~~~

Why `0x400`?  To store the 8 most significant mantissa bits, they need to be rounded first.
Adding this (TODO) guarantees (TODO) they are correctly rounded within 10 bits:

~~~
  0eee eeee eeee mmmm mmmm rrrr rrrr rrrr
+ 0000 0000 0000 0000 0000 0100 0000 0000
  ---------------------------------------
  0eee eeee eeee mmmm mmmm
~~~

TODO

Now, since x87 uses 80-bit floating point internally, and the LUT only stores
the 8 most significant bits of the mantissa, this could easily be recompiled for SSE or something newer.  The
extra precision isn't used.

So now the LUT stores the rounded most significant 8 bits of the `float64` mantissa for `1.0/sqrt(x)`.

In the actual function, we can get the correct 8 bits... if we know which table entry to use.

First, lets assume our original `[0.5 .. 2)` input as it was.  Now we can immediately see that we can simply
index by ... TODO ... to obtain the 8 most significant mantissa bits of the result.

Theorem: ... TODO (take any FP ... )

TODO

OK, so we got our exponent bits.  We got our 8 mantissa bits.  We know the sign bit (0).  Now we can `or` these together to form a 
a complete `float64` number:

TODO

Nice!  But still not done - need to figure out the method at the bottom.  If exclude that, we still have an approximation,
but not a great one. (TODO)

I have a hunch it's one iteration of a Newton-Raphson approximation given the structure of the code, because we already
have a good starting value.

The math for Newton Raphson is ...

TODO

Indeed, the Carmack `Q_rsqrt` function is commonly characterized as being one iteration of Newton-Raphson with a good
first guess.

On paper, we get ...


This looks really close!  But what's the deal with this `1.00001` term?  It appears it is just a slight manual adjustment
to make it a bit better.  Indeed, multiplying by this makes `1.0` map to approximately `1.0`... TODO

The result is another variant of Newton-Raphson, astonishingly close to the Quake code except that the initial
first guess is obtained via a formula to get exponent bits and mantissa bits...

Now let's see how they compare.  We need to consider the range `[0.5 .. 2)`...

Let's iterate a lot of `float32`s in that range... this should be OK because the conversion from `float32` to `float64`
is non-lossy... TODO, rerun

Note that the method could be extended to increase the precision, but each extra bit doubles the size of the LUT!
And it doesn't really make sense to do this in new code. (TODO: warning about never using this on new processors).
But this function would have been a big deal in 1997, and possibly earlier if it was present in the original
Mechwarrior 2 (TODO).  Being able to quickly normalize vectors would make 3D graphics much easier to do
on the CPU, even if the results weren't perfect.

Now, let's put our theory into practice: let us replace this function in I'76 with an upgraded version that is both
faster and more accurate using `SSE2` (3? 4? TODO).

TODO: screenshot comparison

I've put the code on [GitHub](TODO) in case anyone wants to play around with the code.  Thanks for tuning in!
Next up: finding an old bug where ramming your car in Nitro would cause your health to reset (suspect it's an overflow bug).

# Appendix

## IEEE-754 float64 decomposition{#ieee754-decomp}

Let $x \in \mathbb{R}, x \geq 0$.  Then:

{{< katex display >}}
x = 2^\floor{\lg\left(x\right)}*\frac{x}{2^\floor{\lg\left(x\right)}}
{{< /katex >}}

This works because:

{{< katex aligned >}}
& 0 \leq \lg\left(x\right) - \floor{\lg\left(x\right)} < 1 \\
\implies& 2^0 \leq 2^{\lg\left(x\right) - \floor{\lg\left(x\right)}} < 2^1 \\
\implies& 1 \leq \frac{2^{\lg\left(x\right)}}{2^\floor{\lg\left(x\right)}} < 2 \\
\implies& 1 \leq \frac{x}{2^\floor{\lg\left(x\right)}} < 2
{{< /katex >}}

To represent this as a `float64`, two conditions need to be met:

* $-1023 < \floor{\lg\left(x\right)} < 1024$, otherwise this is an overflow or underflow condition (possibly requiring a subnormal)
* $\frac{x}{2^\floor{\lg\left(x\right)}} - 1$ can fit in the mantissa (52 bits), otherwise truncation will occur (least significant bits stripped off)

Handling negative values is simple as well -- treat it as a positive value and then set the sign bit accordingly.

## Alternative formulation of exponent

Now, imagine if we took the reciprocal square root of $v$.  What would it look like?

{{< katex display >}}
\frac{1}{\sqrt{v}} 
= \frac{1}{\sqrt{2^\fexp(1 + \ffrac)}} 
= \frac{1}{\sqrt{2^\fexp}\sqrt{1 + \ffrac}} \\
= 2^{-\fexp/2}*\frac{1}{\sqrt{1 + \ffrac}}
{{< /katex >}}

Now this is interesting.  First, observe the following about: $\frac{1}{\sqrt{1 + \ffrac}}$:

{{< katex aligned >}}
&0 \leq \ffrac < 1 \\
\implies& 1 \leq 1 + \ffrac < 2 \\
\implies& 1 \leq \sqrt{1 + \ffrac} < \sqrt{2} \\
\implies& \frac{1}{\sqrt{2}} < \frac{1}{\sqrt{1 + \ffrac}} \leq 1 \\
\implies& 1 < \sqrt{2} < \frac{2}{\sqrt{1 + \ffrac}} \leq 2
{{< /katex >}}

Let's extract the fractional part of the mantissa above:

{{< katex display >}}
1 + m = \frac{2}{\sqrt{1 + \ffrac}}
{{< /katex >}}

Therefore $0 < m \leq 1$.
This is *very close* to being a valid mantissa for IEEE-754.  There's just the pesky
condition where $\ffrac = 0 \Rightarrow m = 1$.  Let's assume that $\ffrac \neq 0$:

{{< katex aligned  >}}
 &2^{-\fexp/2 - 1}\frac{2}{\sqrt{1 + \ffrac}} \\
 =& 2^{-\fexp/2 - 1}(1 + m) \\
 =& 2^{-(\fexp + 2)/2}(1 + m)
{{< /katex >}}

This is a valid floating point number!  Great!

But what if $\ffrac = 0$?  In this case, we must normalize the number by increasing the exponent by 1 and setting
the fractional part of the mantissa to 0:

{{< katex aligned >}}
 & 2^{-\fexp/2 - 1}(1 + m) \\
 =& 2^{-\fexp/2 - 1}(2) \\
 =& 2^{-\fexp/2}(1 + 0)
{{< /katex >}}

Next, observe that  $-1023 < -\fexp/2 < 1024$.  However, $-\fexp/2$ may not be an integer, unfortunately, but if it *was*,  then
this would be a valid exponent for a `float64`.

TODO ODD
But what if $\fexp$ isn't even?  Some additional analysis is in order.  Assume $\fexp$ is odd -- then $\fexp = 2p + 1$:

 {{< katex aligned >}}

 & 2^{-\fexp/2 - 1}(1 + m) \\
=& 2^{-(\fexp + 1 - 1)/2 - 1}(1 + m) \\
=& 2^{-(\fexp + 1)/2 + 1/2 - 1}(1 + m) \\
=& 2^{-(\fexp + 1)/2 - 1/2}(1 + m) \\
=& 2^{-(\fexp + 1)/2 - 1/2}\frac{2}{\sqrt{1 + \ffrac}} \\
=& 2^{-(\fexp + 1)/2}\frac{2}{\sqrt{2}\sqrt{1 + \ffrac}} \\
=& 2^{-(\fexp + 1)/2}\frac{\sqrt{2}}{\sqrt{1 + \ffrac}} \\
=& 2^{-(\fexp + 1)/2}\frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} \\

 {{< /katex >}}

Note that:

{{< katex display >}}
1 < \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}} \leq \sqrt{2} < 2
{{< /katex >}}

In all cases -- regardless of if $\ffrac = 0$.

If we choose:

{{< katex display >}}
1 + m = \frac{1}{\sqrt{\frac{1 + \ffrac}{2}}}
{{< /katex >}}

Since $0 < m < 1$ and $\fexp + 1$, then:

{{< katex display >}}
 2^{-(\fexp + 1)/2}(1+m)
{{< /katex >}}

is a valid `float64`! TODO: verify mantissa fits... i doubt very much it does

TODO: what happens if mantissa overflows? 0xFFF000 + 0x400... does this ever happen?

TODO: discuss log approach?

TODO NOMENCLATURE: significand includes 1?  adjust representation from (1 + m) to on paper?

Let's recap.  We have three cases:
1. $\fexp$ is odd
2. $\fexp$ is even and $\ffrac = 0$
3. $\fexp$ is even and $\ffrac \neq 0$

Now we can directly determine what the exponent should be for the result.
What would the biased exponent be?

1. $rexp_1 = -(\fexp + 1)/2$
2. $rexp_2 = -(\fexp/2)$
3. $rexp_3 = -(\fexp + 2)/2$

So the actual biased value would be:

1. $bexp_1 = rexp_1 + 1023 = -(\fexp + 1)/2 + 1023 = (2046 - \fexp - 1)/2 = (2045 - \fexp)/2$
2. $bexp_2 = rexp_2 + 1023 = -(\fexp/2) + 1023 = -(\fexp/2) + 2046/2 = (2046 - \fexp)/2$
3. $bexp_3 = rexp_3 + 1023 = -(\fexp/2 - 1) + 1023 = -(\fexp/2 - 1) + 2046/2 = (2044 - \fexp)/2$
